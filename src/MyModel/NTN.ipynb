{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1/+cUNMYvd6x4E9cqXY4m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/src/MyModel/NTN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NTN (Neural Tensor Network) [[reference_paper](https://proceedings.neurips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf)]\n",
        "\n",
        "<u><i>Goal</i></u> : state if two entities $(e_1, e_2)$ are in a certain relationship $R$.   \n",
        ">Ex. defines whehter $$(e_1, R, e_2) = (\\text{Bengal tiger}, \\text{has part}, \\text{tail})$$ is true and with which certainty.\n",
        "\n",
        "- $e_1$ and $e_2$ are vector representations or features of the two entities.\n",
        "- NTN, unlike a linear canoncical NN layer, uses a bilinear tensor layer that directly relates two entity vectors across differet dimensions.\n",
        "- Model computes a score of how likely it is two entities are in a specific position following : $$g(e_1, R, e_2) = u_R^Tf\\biggr(e_i^T W_R^{[1:K]}e_2+V_R \\begin{align}\n",
        "    \\begin{bmatrix}\n",
        "           e_{1} \\\\\n",
        "           e_{2} \\\\\n",
        "         \\end{bmatrix}\n",
        "  \\end{align} + b_R\\Biggl)$$  \n",
        "where : \n",
        "- $f=\\tanh$\n",
        "- $W_R^{[1:K]} \\in \\mathbb{R}^{d\\times d\\times k}$ is a multi-dimensional tensor\n",
        "- $e_1^TW_R^{[1:k]}e_2=h\\in\\mathbb{R}$ is the bilinear tensor\n",
        "- $V_R \\in \\mathbb{R}^{k\\times2d}$, $U \\in \\mathbb{R}^K$, $b_R\\in \\mathbb{R}^K$ are NN parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KuB3DKrgQfSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4WlxO-pC-Pmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorNetwork(nn.Module):\n",
        "    def __init__(self, output_dim: int, input_dim: int, activation: str=\"tanh\", mean: float=0.0, std: float=1.0):\n",
        "        \n",
        "        super(NeuralTensorNetwork, self).__init__()\n",
        "\n",
        "        # setting input and output dimensions\n",
        "        self.k = output_dim\n",
        "        self.d = input_dim # e1,e2\n",
        "\n",
        "        # setting mean and std for random initialization\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        # parameters has been used in order to consider W, V, b as model parameters\n",
        "        # inference -> they'll be optimized\n",
        "\n",
        "        # normal sampling -> https://pytorch.org/docs/stable/generated/torch.normal.html\n",
        "        # parameter -> https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
        "        self.W = nn.Parameter(torch.normal(self.mean, self.std, size=(self.k, self.d, self.d)))\n",
        "        self.V = nn.Parameter(torch.normal(self.mean, self.std, size=(2*self.d, self.k)))\n",
        "        self.b = nn.Parameter(torch.zeros(size=(self.d,)))\n",
        "        \n",
        "        if activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif self.activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        # checking for a good activation function\n",
        "        else:\n",
        "            raise ValueError('Possible activation choices are tanh, sigmoid or ReLU')\n",
        "\n",
        "    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]):\n",
        "\n",
        "        # getting the entities\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "\n",
        "        # input tensor should be of shape (batch_size, padded_length, 768)\n",
        "        batch_size = e1[0]\n",
        "        k = self.k\n",
        "        d = self.d\n",
        "\n",
        "        # bilinear tensor + bias\n",
        "        bil_bias = [torch.sum((e2 * torch.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        for i in range(1,k):\n",
        "            bil_bias.append(torch.sum((e2*torch.dot(e1, self.W[i]))) + self.b, axis=1)\n",
        "        bil_bias = torch.reshape(torch.cat(bil_bias, axis=0), (batch_size, k))\n",
        "\n",
        "        # Vr * [e1, e2]\n",
        "        rest = torch.dot(torch.cat([e1,e2]), self.V)\n",
        "\n",
        "        e1_R_e2 = bil_bias + rest\n",
        "\n",
        "        # applying activation\n",
        "        f = self.activation(e1_R_e2)\n",
        "        return f"
      ],
      "metadata": {
        "id": "-anTmLwuTLvR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batching_data(data: Tuple[np.array, np.array], batch_size: int=64) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
        "    x1 = DataLoader(dataset=data[0], sampler=RandomSampler(data[0]), batch_size=64, shuffle=False)\n",
        "    x2 = DataLoader(dataset=data[1], sampler=RandomSampler(data[1]), batch_size=64, shuffle=False)\n",
        "    return x1, x2 "
      ],
      "metadata": {
        "id": "FtBwT8k8z-89"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "TXq4IXC-pYRW",
        "outputId": "b48f26e3-dd34-4699-f41d-4fd412c70c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = batching_data([x_train1, x_train2])"
      ],
      "metadata": {
        "id": "7xvVymYO2uAk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x1)"
      ],
      "metadata": {
        "id": "XlE35uqf7ItV",
        "outputId": "d207aaa8-0855-460b-a696-d518aa71506a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x2)"
      ],
      "metadata": {
        "id": "7FgpHwh97N1N",
        "outputId": "a3c33fdc-9f83-4248-9e3e-aa8725c1992b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mse() -> torch.nn.MSELoss:\n",
        "    return nn.MSELoss()"
      ],
      "metadata": {
        "id": "MoLLwKrUpo2E"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr):\n",
        "    return torch.optim.Adam(model.parameters(), lr)"
      ],
      "metadata": {
        "id": "Kygj0APpqNMy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, data, optimizer, cf, targets, device='cuda'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "  \n",
        "    model.train() \n",
        " \n",
        "    # iterate over the training set\n",
        "    for batch_idx, inputs in enumerate(data):\n",
        "        # load data into GPU\n",
        "        input1 = inputs[0].to(device)\n",
        "        input2 = inputs[1].to(device)\n",
        "        inputs = [input1, input2]\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # loss computation\n",
        "        loss = get_mse(outputs, targets)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "    \n",
        "        # parameters update\n",
        "        optimizer.step()\n",
        "\n",
        "        # gradients reset\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # fetch prediction and loss value\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "        # compute training accuracy\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "UWog0GYEq7fD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    model = NeuralTensorNetwork(output_dim=32, input_dim=300, activation='relu').to('cuda')\n",
        "    optimizer = get_optimizer(model, 0.001)\n",
        "    loss = get_mse()\n",
        "\n",
        "    data = batching_data([x_train1, x_train2])\n",
        "\n",
        "    for e in range(0,5):\n",
        "        train_loss, train_accuracy = training_step(model, data, optimizer, loss, y_train, 'cuda')\n",
        "        print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n"
      ],
      "metadata": {
        "id": "hPV92aDducGF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "3yg72_9i1BSk",
        "outputId": "104d7a3a-b72d-4852-d68a-584ee00b6d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-489d983485c9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-66743d9c4e04>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, data, optimizer, cf, targets, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# load data into GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0minput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "2zWIRcpstrr1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorLayer(Layer):\n",
        "    def __init__(self, output_dim, input_dim, activation= None):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim #The k in the formula\n",
        "        self.input_dim = input_dim   #The d in the formula\n",
        "        self.activation = activation #The f function in the formula\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        #The initialisation parameters\n",
        "        self.mean = 0.0 \n",
        "        self.stddev = 1.0\n",
        "        dtype = 'float32'\n",
        "        self.seed = 1\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "        \n",
        "        #Initialise the variables to be trained. The variables are according to the\n",
        "        #function defined.\n",
        "        self.W = K.variable(K.random_normal((k,d,d), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.V = K.variable(K.random_normal((2*d,k), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.b = K.zeros((self.input_dim,))\n",
        "        \n",
        "        #Set the variables to be trained.\n",
        "        self._trainable_weights = [self.W, self.V, self.b]\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        #Get Both the inputs\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "        \n",
        "        #Get the batch size\n",
        "        batch_size = K.shape(e1)[0]\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "\n",
        "        #The first term in the function which is the bilinear product is calculated here.\n",
        "        first_term_k = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        for i in range(1, k):\n",
        "            temp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
        "            first_term_k.append(temp)\n",
        "        first_term = K.reshape(K.concatenate(first_term_k, axis=0), (batch_size, k))\n",
        "\n",
        "        #The second term in the function is calculated here.\n",
        "        second_term = K.dot(K.concatenate([e1,e2]), self.V)\n",
        "        \n",
        "        #Sum of the two terms to get the final function\n",
        "        z =  first_term + second_term\n",
        "        \n",
        "        #The activation is selected here\n",
        "        if (self.activation == None):\n",
        "            return z\n",
        "        elif (self.activation == 'tanh'):\n",
        "            return K.tanh(z)\n",
        "        elif (self.activation == 'relu'):\n",
        "            return K.relu(z)\n",
        "        else :\n",
        "            print ('Activation not found')\n"
      ],
      "metadata": {
        "id": "7c9WtJoGtl1b"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)\n"
      ],
      "metadata": {
        "id": "YxiWrtEXuoJ7",
        "outputId": "24b348dc-4117-4c4b-f11b-58bafcf2af71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here Define the model\n",
        "vector1 = Input(shape=(300,), dtype='float32')\n",
        "vector2 = Input(shape=(300,), dtype='float32')\n",
        "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=300, \n",
        "                                  activation= 'relu')([vector1, vector2])\n",
        "\n",
        "g = Dense(1)(BilinearLayer)\n",
        "#The g or the output of the modelled function.\n",
        "model = Model(inputs=[vector1, vector2], outputs=[g])\n",
        "\n",
        "#Compile the model\n",
        "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile( loss='mean_squared_error', optimizer=adam)\n",
        "#The summary of the model.\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ssnK68owuovX",
        "outputId": "f59795b8-76a6-4fb6-a046-1612d429c795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " neural_tensor_layer_9 (NeuralT  (None, 32)          2899500     ['input_21[0][0]',               \n",
            " ensorLayer)                                                      'input_22[0][0]']               \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            33          ['neural_tensor_layer_9[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,899,533\n",
            "Trainable params: 2,899,533\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([x_train1, x_train2], y_train,\n",
        "          batch_size=64, epochs=5,\n",
        "          validation_data=([x_val1, x_val2], y_val))"
      ],
      "metadata": {
        "id": "8uGv2VC0uz_k",
        "outputId": "33d4549e-e18f-4f12-ca46-e25296e475de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "16/16 [==============================] - 3s 42ms/step - loss: 515.0410 - val_loss: 0.4053\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.7228 - val_loss: 0.4059\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3627 - val_loss: 0.3962\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3516 - val_loss: 0.3828\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3382 - val_loss: 0.3679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f065397ad50>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHOYAvffxwwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}