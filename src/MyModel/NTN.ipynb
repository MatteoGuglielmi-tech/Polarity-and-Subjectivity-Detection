{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/src/MyModel/NTN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NTN (Neural Tensor Network) [[reference_paper](https://proceedings.neurips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf)]\n",
        "\n",
        "<u><i>Goal</i></u> : state if two entities $(e_1, e_2)$ are in a certain relationship $R$.   \n",
        ">Ex. defines whehter $$(e_1, R, e_2) = (\\text{Bengal tiger}, \\text{has part}, \\text{tail})$$ is true and with which certainty.\n",
        "\n",
        "- $e_1$ and $e_2$ are vector representations or features of the two entities.\n",
        "- NTN, unlike a linear canoncical NN layer, uses a bilinear tensor layer that directly relates two entity vectors across differet dimensions.\n",
        "- Model computes a score of how likely it is two entities are in a specific position following : $$g(e_1, R, e_2) = u_R^Tf\\biggr(e_i^T W_R^{[1:K]}e_2+V_R \\begin{align}\n",
        "    \\begin{bmatrix}\n",
        "           e_{1} \\\\\n",
        "           e_{2} \\\\\n",
        "         \\end{bmatrix}\n",
        "  \\end{align} + b_R\\Biggl)$$  \n",
        "where : \n",
        "- $f=\\tanh$\n",
        "- $W_R^{[1:K]} \\in \\mathbb{R}^{d\\times d\\times k}$ is a multi-dimensional tensor\n",
        "- $e_1^TW_R^{[1:k]}e_2=h\\in\\mathbb{R}$ is the bilinear tensor\n",
        "- $V_R \\in \\mathbb{R}^{k\\times2d}$, $U \\in \\mathbb{R}^K$, $b_R\\in \\mathbb{R}^K$ are NN parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KuB3DKrgQfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### pytorch trials"
      ],
      "metadata": {
        "id": "Tq-h_R_lEKwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4WlxO-pC-Pmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorNetwork(nn.Module):\n",
        "    def __init__(self, output_dim: int, input_dim: int, activation: str=\"tanh\", mean: float=0.0, std: float=1.0):\n",
        "        \n",
        "        super(NeuralTensorNetwork, self).__init__()\n",
        "\n",
        "        # setting input and output dimensions\n",
        "        self.k = output_dim\n",
        "        self.d = input_dim # e1,e2\n",
        "\n",
        "        # setting mean and std for random initialization\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        # parameters has been used in order to consider W, V, b as model parameters\n",
        "        # inference -> they'll be optimized\n",
        "\n",
        "        # normal sampling -> https://pytorch.org/docs/stable/generated/torch.normal.html\n",
        "        # parameter -> https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
        "        self.W = nn.Parameter(torch.normal(self.mean, self.std, size=(self.k, self.d, self.d)))\n",
        "        self.V = nn.Parameter(torch.normal(self.mean, self.std, size=(2*self.d, self.k)))\n",
        "        self.b = nn.Parameter(torch.zeros(size=(self.d,)))\n",
        "        \n",
        "        if activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif self.activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        # checking for a good activation function\n",
        "        else:\n",
        "            raise ValueError('Possible activation choices are tanh, sigmoid or ReLU')\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
        "\n",
        "        # getting the entities\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "\n",
        "        torch.print(e1)\n",
        "        torch.print(e2)\n",
        "\n",
        "        # input tensor should be of shape (batch_size, padded_length, 768)\n",
        "        batch_size = e1[0]\n",
        "        k = self.k\n",
        "        d = self.d\n",
        "\n",
        "        # bilinear tensor + bias\n",
        "        bil_bias = [torch.sum((e2 * torch.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        for i in range(1,k):\n",
        "            bil_bias.append(torch.sum((e2*torch.dot(e1, self.W[i]))) + self.b, axis=1)\n",
        "        bil_bias = torch.reshape(torch.cat(bil_bias, axis=0), (batch_size, k))\n",
        "\n",
        "        # Vr * [e1, e2]\n",
        "        rest = torch.dot(torch.cat([e1,e2]), self.V)\n",
        "\n",
        "        e1_R_e2 = bil_bias + rest\n",
        "\n",
        "        # applying activation\n",
        "        f = self.activation(e1_R_e2)\n",
        "        return f"
      ],
      "metadata": {
        "id": "-anTmLwuTLvR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batching_data(dataset, batch_size: int=64) -> torch.utils.data.DataLoader:\n",
        "    dataloader = DataLoader(dataset=dataset, sampler=RandomSampler(dataset), batch_size=batch_size, shuffle=False)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "FtBwT8k8z-89"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = []\n",
        "i = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "ll.append(i)\n",
        "for k in range(0,10):\n",
        "    ll.append(np.zeros(i.shape))\n",
        "\n",
        "print(ll)\n",
        "\n",
        "\n",
        "l = []\n",
        "\n",
        "j = np.array([11,12,13,14,15,16,17,18,19,20])\n",
        "l.append(j)\n",
        "for z in range(0,10):\n",
        "    l.append(np.ones(j.shape))\n",
        "\n",
        "print(l)\n",
        "\n",
        "print(np.array(l).shape)\n",
        "print(np.array(ll).shape)\n",
        "\n",
        "y = np.random.random((len(l),1))\n",
        "y.shape"
      ],
      "metadata": {
        "outputId": "6a8b18bd-9baf-447b-e794-9180e524c1ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qxlLJz7YbBm"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "[array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
            "(11, 10)\n",
            "(11, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dataloader = batching_data(np.array(ll), batch_size=4)\n",
        "e2_dataloader = batching_data(np.array(l), batch_size=4)"
      ],
      "metadata": {
        "id": "jVIm2yo9gzLB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dataloader"
      ],
      "metadata": {
        "id": "BABMrSGzhU_t",
        "outputId": "05cd17de-fe05-4f89-848c-91a119134550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f2dc32fa0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e2_dataloader"
      ],
      "metadata": {
        "id": "oc9Gy7bdjfKe",
        "outputId": "f3c23a99-16e6-46f8-b5c2-35f39eccb159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f2dc32f74d0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(e1_dataloader))\n",
        "\n",
        "for i in range (len(iter(e1_dataloader))):\n",
        "  print(next(iter(e1_dataloader)))"
      ],
      "metadata": {
        "id": "_6N1FEmri_4R",
        "outputId": "6baf3fc9-fbe2-4964-e9f7-d33fe3523e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(e2_dataloader))\n",
        "\n",
        "for i in range (len(iter(e2_dataloader))):\n",
        "  print(next(iter(e2_dataloader)))"
      ],
      "metadata": {
        "id": "cNI4NePvhf1t",
        "outputId": "da4f14ae-7016-476f-c1c5-bb17712046c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [11., 12., 13., 14., 15., 16., 17., 18., 19., 20.],\n",
            "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [11., 12., 13., 14., 15., 16., 17., 18., 19., 20.]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, e1_dataloader, e2_dataloader, optimizer, device='cuda'):\n",
        "\n",
        "  # converting the two dataloaders in iterators\n",
        "  e1_iterator = iter(e1_dataloader)\n",
        "  e2_iterator = iter(e2_dataloader)\n",
        "\n",
        "  # getting lengths \n",
        "  iter1_length = len(e1_iterator)\n",
        "  iter2_length = len(e2_iterator)\n",
        "\n",
        "  flag = 0\n",
        "\n",
        "  if iter1_length > iter2_length:\n",
        "    flag = 1\n",
        "  elif iter2_length > iter1_length:\n",
        "    flag = 2\n",
        "\n",
        "  if flag==1:\n",
        "    for i in range(iter2_length):\n",
        "      # if the second entity's vector is longer than the first one\n",
        "      # I need to deal with this case\n",
        "      # With the devised solution, once the first iterator has reached the end \n",
        "      # it will be reinitialized \n",
        "      try:\n",
        "        e1_input = next(e1_iterator)\n",
        "        e1_input = e1_input.to(device)\n",
        "      except:\n",
        "        e1_iterator = iter(e1_dataloader)\n",
        "        e1_input = next(e1_iterator)\n",
        "        e1_input = e1_input.to(device)\n",
        "\n",
        "  elif flag==2:\n",
        "    for i in range(iter1_length):\n",
        "      try:\n",
        "        e2_input = next(e2_iterator)\n",
        "        e2_input = e2_input.to(device)\n",
        "      except:\n",
        "        e2_iterator = iter(e2_dataloader)\n",
        "        e2_input = next(e2_iterator)\n",
        "        e2_input = e2_input.to(device)\n",
        "  \n",
        "  outpu = model(input)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3yhW3NFHjsnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### to collapse"
      ],
      "metadata": {
        "id": "bfw_Ard1YVR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "TXq4IXC-pYRW",
        "outputId": "3c92b12f-ca50-45f5-8aad-36884859581a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = batching_data([x_train1, x_train2])"
      ],
      "metadata": {
        "id": "7xvVymYO2uAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlE35uqf7ItV",
        "outputId": "d207aaa8-0855-460b-a696-d518aa71506a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FgpHwh97N1N",
        "outputId": "a3c33fdc-9f83-4248-9e3e-aa8725c1992b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mse() -> torch.nn.MSELoss:\n",
        "    return nn.MSELoss()"
      ],
      "metadata": {
        "id": "MoLLwKrUpo2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr):\n",
        "    return torch.optim.Adam(model.parameters(), lr)"
      ],
      "metadata": {
        "id": "Kygj0APpqNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    bilinear : NeuralTensorNetwork(output_dim=32, input_dim=300, activation='relu')\n",
        "    g : nn.Linear(in_features=32, out_features=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        bilinear = bilinear(x)\n",
        "        dense = g(bilinear)\n",
        "\n",
        "        return dense\n"
      ],
      "metadata": {
        "id": "oXYJHYFm_Fqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, data, optimizer, cf, targets, device='cuda'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "  \n",
        "    model.train() \n",
        " \n",
        "    # iterate over the training set\n",
        "    for batch_idx, inputs in enumerate(data):\n",
        "        # load data into GPU\n",
        "        input1 = inputs[0].to(device)\n",
        "        input2 = inputs[1].to(device)\n",
        "        inputs = [input1, input2]\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # loss computation\n",
        "        loss = get_mse(outputs, targets)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "    \n",
        "        # parameters update\n",
        "        optimizer.step()\n",
        "\n",
        "        # gradients reset\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # fetch prediction and loss value\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "        # compute training accuracy\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "UWog0GYEq7fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    model = NeuralTensorNetwork(output_dim=32, input_dim=300, activation='relu').to('cuda')\n",
        "    optimizer = get_optimizer(model, 0.001)\n",
        "    loss = get_mse()\n",
        "\n",
        "    data = batching_data([x_train1, x_train2])\n",
        "\n",
        "    for e in range(0,5):\n",
        "        train_loss, train_accuracy = training_step(model, data, optimizer, loss, y_train, 'cuda')\n",
        "        print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n"
      ],
      "metadata": {
        "id": "hPV92aDducGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "3yg72_9i1BSk",
        "outputId": "104d7a3a-b72d-4852-d68a-584ee00b6d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-489d983485c9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-66743d9c4e04>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, data, optimizer, cf, targets, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# load data into GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0minput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## to keep"
      ],
      "metadata": {
        "id": "iSNs5M6DYycb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensrflow Version"
      ],
      "metadata": {
        "id": "XYzKkf7VEREB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K  # in keras simple computations are not handled directly but it relies on a well optimized tensor handler library\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "2zWIRcpstrr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorLayer(Layer):\n",
        "    def __init__(self, output_dim, input_dim, activation= None):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim #The k in the formula\n",
        "        self.input_dim = input_dim   #The d in the formula\n",
        "        self.activation = activation #The f function in the formula\n",
        "        \n",
        "    # called the first time call is called\n",
        "    def build(self, input_shape):\n",
        "        #The initialisation parameters\n",
        "        self.mean = 0.0 \n",
        "        self.stddev = 1.0\n",
        "        dtype = 'float32'\n",
        "        self.seed = 1\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "        \n",
        "        #Initialise the variables to be trained. The variables are according to the\n",
        "        #function defined.\n",
        "        self.W = K.variable(K.random_normal((k,d,d), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.V = K.variable(K.random_normal((2*d,k), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.b = K.zeros((self.input_dim,))\n",
        "        \n",
        "        #Set the variables to be trained.\n",
        "        self._trainable_weights = [self.W, self.V, self.b]\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        #Get Both the inputs\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "\n",
        "        tf.print(e1)\n",
        "        tf.print(e2)\n",
        "\n",
        "        #Get the batch size\n",
        "        batch_size = K.shape(e1)[0]\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "\n",
        "        #The first term in the function which is the bilinear product is calculated here.\n",
        "        first_term_k = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        for i in range(1, k):\n",
        "            temp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
        "            first_term_k.append(temp)\n",
        "        first_term = K.reshape(K.concatenate(first_term_k, axis=0), (batch_size, k))\n",
        "\n",
        "        #The second term in the function is calculated here.\n",
        "        second_term = K.dot(K.concatenate([e1,e2]), self.V)\n",
        "        \n",
        "        #Sum of the two terms to get the final function\n",
        "        z =  first_term + second_term\n",
        " \n",
        "        # The activation is selected here\n",
        "        if (self.activation == None):\n",
        "            return z\n",
        "        elif (self.activation == 'tanh'):\n",
        "            return K.tanh(z)\n",
        "        elif (self.activation == 'relu'):\n",
        "            return K.relu(z)\n",
        "        else :\n",
        "            print ('Activation not found')"
      ],
      "metadata": {
        "id": "7c9WtJoGtl1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxiWrtEXuoJ7",
        "outputId": "c1cca00d-0307-48ab-f180-3a7c4d0599c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here Define the model\n",
        "vector1 = Input(shape=(10,), dtype='float32')\n",
        "vector2 = Input(shape=(10,), dtype='float32')\n",
        "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=10, \n",
        "                                  activation= 'relu')([vector1, vector2])\n",
        "\n",
        "g = Dense(1)(BilinearLayer)\n",
        "\n",
        "#The g or the output of the modelled function.\n",
        "model = Model(inputs=[vector1, vector2], outputs=[g])\n",
        "\n",
        "#Compile the model\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "#The summary of the model.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssnK68owuovX",
        "outputId": "22d00b79-4cab-421c-87df-5a659b48453c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " neural_tensor_layer_17 (Neural  (None, 32)          3850        ['input_35[0][0]',               \n",
            " TensorLayer)                                                     'input_36[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 1)            33          ['neural_tensor_layer_17[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,883\n",
            "Trainable params: 3,883\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([x_train1, x_train2], y_train,\n",
        "          batch_size=64, epochs=10,\n",
        "          validation_data=([x_val1, x_val2], y_val))"
      ],
      "metadata": {
        "id": "8uGv2VC0uz_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "SHOYAvffxwwl",
        "outputId": "48caa036-93eb-484a-bb86-a852b5801a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ll = []\n",
        "i = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "ll.append(i)\n",
        "for k in range(0,10):\n",
        "    ll.append(np.zeros(i.shape))\n",
        "\n",
        "print(ll)\n",
        "\n",
        "\n",
        "l = []\n",
        "\n",
        "j = np.array([11,12,13,14,15,16,17,18,19,20])\n",
        "l.append(j)\n",
        "for z in range(0,10):\n",
        "    l.append(np.ones(j.shape))\n",
        "\n",
        "print(l)\n",
        "\n",
        "print(np.array(l).shape)\n",
        "print(np.array(ll).shape)\n",
        "\n",
        "y = np.random.random((len(l),1))\n",
        "y.shape"
      ],
      "metadata": {
        "id": "k_8bE3iA_q8U",
        "outputId": "85489be2-757d-45b4-ca17-36d5ee4e777c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "[array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
            "(11, 10)\n",
            "(11, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(([np.array(ll), np.array(l)]), y, batch_size=4, epochs=10)"
      ],
      "metadata": {
        "id": "gpLKGofgrq29",
        "outputId": "36ebea57-86e7-40a6-a9f0-35ff27c0582c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 24.9693[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 55296.9531\n",
            "Epoch 2/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 22.0459[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 52723.6250\n",
            "Epoch 3/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 24.0815[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 49257.1250\n",
            "Epoch 4/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 232299.7969[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 84486.7422\n",
            "Epoch 5/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 23.1153[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 29888.4434\n",
            "Epoch 6/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 20.3125[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 79863.0469\n",
            "Epoch 7/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 214036.5938[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 77845.0703\n",
            "Epoch 8/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 23.1210[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 37405.0781\n",
            "Epoch 9/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 21.8295[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70559.7266\n",
            "Epoch 10/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 21.3914[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 33952.4648\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0581110f50>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_Bll6SNu12J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}