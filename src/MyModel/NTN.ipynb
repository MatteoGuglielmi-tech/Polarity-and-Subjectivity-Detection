{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/src/MyModel/NTN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NTN (Neural Tensor Network) [[reference_paper](https://proceedings.neurips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf)]\n",
        "\n",
        "<u><i>Goal</i></u> : state if two entities $(e_1, e_2)$ are in a certain relationship $R$.   \n",
        ">Ex. defines whehter $$(e_1, R, e_2) = (\\text{Bengal tiger}, \\text{has part}, \\text{tail})$$ is true and with which certainty.\n",
        "\n",
        "- $e_1$ and $e_2$ are vector representations or features of the two entities.\n",
        "- NTN, unlike a linear canoncical NN layer, uses a bilinear tensor layer that directly relates two entity vectors across differet dimensions.\n",
        "- Model computes a score of how likely it is two entities are in a specific position following : $$g(e_1, R, e_2) = u_R^Tf\\biggr(e_i^T W_R^{[1:K]}e_2+V_R \\begin{align}\n",
        "    \\begin{bmatrix}\n",
        "           e_{1} \\\\\n",
        "           e_{2} \\\\\n",
        "         \\end{bmatrix}\n",
        "  \\end{align} + b_R\\Biggl)$$  \n",
        "where : \n",
        "- $f=\\tanh$\n",
        "- $W_R^{[1:K]} \\in \\mathbb{R}^{d\\times d\\times k}$ is a multi-dimensional tensor\n",
        "- $e_1^TW_R^{[1:k]}e_2=h\\in\\mathbb{R}$ is the bilinear tensor\n",
        "- $V_R \\in \\mathbb{R}^{k\\times2d}$, $U \\in \\mathbb{R}^K$, $b_R\\in \\mathbb{R}^K$ are NN parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KuB3DKrgQfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### pytorch trials"
      ],
      "metadata": {
        "id": "Tq-h_R_lEKwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4WlxO-pC-Pmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorNetwork(nn.Module):\n",
        "    def __init__(self, output_dim: int, input_dim: int, activation: str=\"tanh\", mean: float=0.0, std: float=1.0):\n",
        "        \n",
        "        super(NeuralTensorNetwork, self).__init__()\n",
        "\n",
        "        # setting input and output dimensions\n",
        "        self.k = output_dim\n",
        "        self.d = input_dim # e1,e2\n",
        "\n",
        "        # setting mean and std for random initialization\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        # parameters has been used in order to consider W, V, b as model parameters\n",
        "        # inference -> they'll be optimized\n",
        "\n",
        "        # normal sampling -> https://pytorch.org/docs/stable/generated/torch.normal.html\n",
        "        # parameter -> https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
        "        self.W = nn.Parameter(torch.normal(self.mean, self.std, size=(self.k, self.d, self.d)))\n",
        "        self.V = nn.Parameter(torch.normal(self.mean, self.std, size=(2*self.d, self.k)))\n",
        "        self.b = nn.Parameter(torch.zeros(size=(self.d,)))\n",
        "        \n",
        "        print(f\"self.W : {torch.Tensor.size(self.W)}\")\n",
        "        print(f\"self.V : {torch.Tensor.size(self.V)}\")\n",
        "        print(f\"self.b : {torch.Tensor.size(self.b)}\")\n",
        "        \n",
        "        if activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif self.activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        # checking for a good activation function\n",
        "        else:\n",
        "            raise ValueError('Possible activation choices are tanh, sigmoid or ReLU')\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n",
        "\n",
        "        # getting the entities\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "        print(f\"e1.shape : {torch.Tensor.size(e1)}\")\n",
        "        print(f\"e2.shape : {torch.Tensor.size(e2)}\")\n",
        "        print(f\"self.W[0] : {torch.Tensor.size(self.W[0])}\")\n",
        "        print(f\"self.V : {torch.Tensor.size(self.V)}\")\n",
        "        print(f\"self.bias : {torch.Tensor.size(self.b)}\")\n",
        "\n",
        "        # input tensor should be of shape (batch_size, padded_length, 768)\n",
        "        batch_size = e1[0]\n",
        "        k = self.k\n",
        "        d = self.d\n",
        "\n",
        "        print(f\"dot prod : {torch.matmul(e1, self.W[0])}\")\n",
        "        print(f\"dot prod size: {torch.Tensor.size(torch.matmul(e1, self.W[0]))}\")\n",
        "\n",
        "        # bilinear tensor + bias\n",
        "        #bil_bias = [torch.sum((e2 * torch.tensordot(e1, self.W[0], dims=(batch_size, d))) + self.b, axis=1)]\n",
        "        \n",
        "        #for i in range(1,k):\n",
        "        #    bil_bias.append(torch.sum((e2*torch.tensordot(e1, self.W[i], dims=1))) + self.b, axis=1)\n",
        "        #bil_bias = torch.reshape(torch.cat(bil_bias, axis=0), (batch_size, k))\n",
        "\n",
        "        # Vr * [e1, e2]\n",
        "        #rest = torch.dot(torch.cat([e1,e2]), self.V)\n",
        "\n",
        "        #e1_R_e2 = bil_bias + rest\n",
        "\n",
        "        # applying activation\n",
        "        #f = self.activation(e1_R_e2)\n",
        "        #return f"
      ],
      "metadata": {
        "id": "-anTmLwuTLvR"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batching_data(dataset, batch_size: int=64) -> torch.utils.data.DataLoader:\n",
        "    dataloader = DataLoader(dataset=dataset, sampler=RandomSampler(dataset), batch_size=batch_size, shuffle=False)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "FtBwT8k8z-89"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = []\n",
        "i = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "ll.append(i)\n",
        "for k in range(0,10):\n",
        "    ll.append(np.zeros(i.shape))\n",
        "\n",
        "print(ll)\n",
        "\n",
        "\n",
        "l = []\n",
        "\n",
        "j = np.array([11,12,13,14,15,16,17,18,19,20])\n",
        "l.append(j)\n",
        "for z in range(0,10):\n",
        "    l.append(np.ones(j.shape))\n",
        "\n",
        "print(l)\n",
        "\n",
        "print(np.array(l).shape)\n",
        "print(np.array(ll).shape)\n",
        "\n",
        "y = np.random.random((len(l),1))\n",
        "y.shape"
      ],
      "metadata": {
        "outputId": "420b068c-db7b-4734-a526-70f15a96c4f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qxlLJz7YbBm"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "[array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
            "(11, 10)\n",
            "(11, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dataloader = batching_data(np.array(ll), batch_size=4)\n",
        "e2_dataloader = batching_data(np.array(l), batch_size=4)"
      ],
      "metadata": {
        "id": "jVIm2yo9gzLB"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dataloader"
      ],
      "metadata": {
        "id": "BABMrSGzhU_t",
        "outputId": "4936344c-18ba-4d8c-91b6-d54e5f98e9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f408e632c10>"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e2_dataloader"
      ],
      "metadata": {
        "id": "oc9Gy7bdjfKe",
        "outputId": "ef439b04-b74b-4483-f3fd-d4f7e9971852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f408e6a5110>"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, i in enumerate(iter(e2_dataloader)):\n",
        "    for element in i :\n",
        "        print(idx, element)"
      ],
      "metadata": {
        "id": "TL4SNMYbOjJd",
        "outputId": "13fcdf3e-bcd7-4852-f1fe-a3811ade4fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "2 tensor([11., 12., 13., 14., 15., 16., 17., 18., 19., 20.], dtype=torch.float64)\n",
            "2 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "2 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(e1_dataloader))\n",
        "\n",
        "for i in range (len(iter(e1_dataloader))):\n",
        "  print(next(iter(e1_dataloader)))"
      ],
      "metadata": {
        "id": "_6N1FEmri_4R",
        "outputId": "53998df8-3cc8-43d5-b9b3-761abda9c8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(e2_dataloader))\n",
        "\n",
        "for i in range (len(iter(e2_dataloader))):\n",
        "  print(next(iter(e2_dataloader)))"
      ],
      "metadata": {
        "id": "cNI4NePvhf1t",
        "outputId": "4eed2640-2ae1-49d2-8304-ccfd9c7d8ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [11., 12., 13., 14., 15., 16., 17., 18., 19., 20.],\n",
            "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mse() -> torch.nn.MSELoss:\n",
        "    return nn.MSELoss()"
      ],
      "metadata": {
        "id": "MoLLwKrUpo2E"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr):\n",
        "    return torch.optim.Adam(model.parameters(), lr)"
      ],
      "metadata": {
        "id": "Kygj0APpqNMy"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, e1_dataloader, e2_dataloader, device='cuda'):\n",
        "    # converting the two dataloaders in iterators\n",
        "    print('training')\n",
        "    e1_iterator = iter(e1_dataloader)\n",
        "    e2_iterator = iter(e2_dataloader)\n",
        "\n",
        "    # getting lengths \n",
        "    iter1_length = len(e1_iterator)\n",
        "    iter2_length = len(e2_iterator)\n",
        "\n",
        "    flag = 0\n",
        "    loss = get_mse()\n",
        "    optimizer = get_optimizer(model, 0.001)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if iter1_length > iter2_length:\n",
        "        flag = 1\n",
        "    elif iter2_length > iter1_length:\n",
        "        flag = 2\n",
        "\n",
        "    if flag==1:\n",
        "        for i in range(iter2_length):\n",
        "        # if the second entity's vector is longer than the first one\n",
        "        # I need to deal with this case\n",
        "        # With the devised solution, once the first iterator has reached the end \n",
        "        # it will be reinitialized \n",
        "            try:\n",
        "                e1_input = next(e1_iterator)\n",
        "                e1_input = e1_input.to(device)\n",
        "            except:\n",
        "                e1_iterator = iter(e1_dataloader)\n",
        "                e1_input = next(e1_iterator)\n",
        "                e1_input = e1_input.to(device)\n",
        "\n",
        "            # at this point, al inputs are loaded on GPU\n",
        "            outputs = model([e1_input, e2_input]) # call method\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print(f'outputs: {outputs}')\n",
        "\n",
        "    elif flag==2:\n",
        "        for i in range(iter1_length):\n",
        "            try:\n",
        "                e2_input = next(e2_iterator)\n",
        "                e2_input = e2_input.to(device)\n",
        "            except:\n",
        "                e2_iterator = iter(e2_dataloader)\n",
        "                e2_input = next(e2_iterator)\n",
        "                e2_input = e2_input.to(device)\n",
        "\n",
        "            outputs = model([e1_input, e2_input]) # call method\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    elif flag==0:\n",
        "        for e1, e2 in zip(e1_iterator, e2_iterator):\n",
        "            #for element1, element2 in zip(e1,e2):\n",
        "                # in case they have the same length\n",
        "                #for i in range(iter1_length):\n",
        "            #    e1_input = element1.to(device)\n",
        "            #    e2_input = element2.to(device)\n",
        "                #print(e1_input)\n",
        "                #print(e2_input)\n",
        "            #    outputs = model([e1_input, e2_input]) # forward method\n",
        "        \n",
        "            #    loss.backward()\n",
        "            #    optimizer.step()\n",
        "            #    optimizer.zero_grad()\n",
        "            \n",
        "            # in this way I'm extracting the batches\n",
        "            print(f\"e1 : {e1.shape}\")\n",
        "            print(f\"e2 : {e2.shape}\")\n",
        "\n",
        "            e1_input = e1.to(device)\n",
        "            e2_input = e2.to(device)\n",
        "            outputs = model([e1_input, e2_input]) # forward method\n",
        "\n",
        "\n",
        "            #loss.backward()\n",
        "            #optimizer.step()\n",
        "            #optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "3yhW3NFHjsnr"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(dataloaders, epochs=5, device='cuda'):\n",
        "    model = NeuralTensorNetwork(output_dim=32, input_dim=10)\n",
        "    model = model.double().to(device)\n",
        "    for e in range(epochs):\n",
        "    # def training_step(model, e1_dataloader, e2_dataloader, device='cuda'):\n",
        "        training_step(model, dataloaders[0], dataloaders[1]) \n",
        "        print(e)\n",
        "    print('Traning done')"
      ],
      "metadata": {
        "id": "8_HKAPHSHOsx"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main([e1_dataloader, e2_dataloader])"
      ],
      "metadata": {
        "id": "MUzSiL92JgIP",
        "outputId": "d1c47ad4-ad06-4d78-ebaf-374a106519f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.W : torch.Size([32, 10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.b : torch.Size([10])\n",
            "training\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[-10.0470,  35.9224,  29.1998,  21.6109,  25.7325,  17.0646, -20.7575,\n",
            "          11.5611, -23.2967,  31.7857],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([3, 10])\n",
            "e2 : torch.Size([3, 10])\n",
            "e1.shape : torch.Size([3, 10])\n",
            "e2.shape : torch.Size([3, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([3, 10])\n",
            "0\n",
            "training\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[-10.0470,  35.9224,  29.1998,  21.6109,  25.7325,  17.0646, -20.7575,\n",
            "          11.5611, -23.2967,  31.7857],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([3, 10])\n",
            "e2 : torch.Size([3, 10])\n",
            "e1.shape : torch.Size([3, 10])\n",
            "e2.shape : torch.Size([3, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([3, 10])\n",
            "1\n",
            "training\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [-10.0470,  35.9224,  29.1998,  21.6109,  25.7325,  17.0646, -20.7575,\n",
            "          11.5611, -23.2967,  31.7857],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([3, 10])\n",
            "e2 : torch.Size([3, 10])\n",
            "e1.shape : torch.Size([3, 10])\n",
            "e2.shape : torch.Size([3, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([3, 10])\n",
            "2\n",
            "training\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [-10.0470,  35.9224,  29.1998,  21.6109,  25.7325,  17.0646, -20.7575,\n",
            "          11.5611, -23.2967,  31.7857],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([3, 10])\n",
            "e2 : torch.Size([3, 10])\n",
            "e1.shape : torch.Size([3, 10])\n",
            "e2.shape : torch.Size([3, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([3, 10])\n",
            "3\n",
            "training\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([4, 10])\n",
            "e2 : torch.Size([4, 10])\n",
            "e1.shape : torch.Size([4, 10])\n",
            "e2.shape : torch.Size([4, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000],\n",
            "        [-10.0470,  35.9224,  29.1998,  21.6109,  25.7325,  17.0646, -20.7575,\n",
            "          11.5611, -23.2967,  31.7857],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([4, 10])\n",
            "e1 : torch.Size([3, 10])\n",
            "e2 : torch.Size([3, 10])\n",
            "e1.shape : torch.Size([3, 10])\n",
            "e2.shape : torch.Size([3, 10])\n",
            "self.W[0] : torch.Size([10, 10])\n",
            "self.V : torch.Size([20, 32])\n",
            "self.bias : torch.Size([10])\n",
            "dot prod : tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
            "       dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "dot prod size: torch.Size([3, 10])\n",
            "4\n",
            "Traning done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### to collapse"
      ],
      "metadata": {
        "id": "bfw_Ard1YVR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "TXq4IXC-pYRW",
        "outputId": "3c92b12f-ca50-45f5-8aad-36884859581a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = batching_data([x_train1, x_train2])"
      ],
      "metadata": {
        "id": "7xvVymYO2uAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlE35uqf7ItV",
        "outputId": "d207aaa8-0855-460b-a696-d518aa71506a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FgpHwh97N1N",
        "outputId": "a3c33fdc-9f83-4248-9e3e-aa8725c1992b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensrflow Version"
      ],
      "metadata": {
        "id": "XYzKkf7VEREB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K  # in keras simple computations are not handled directly but it relies on a well optimized tensor handler library\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "2zWIRcpstrr1"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorLayer(Layer):\n",
        "    def __init__(self, output_dim, input_dim, activation= None):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim #The k in the formula\n",
        "        self.input_dim = input_dim   #The d in the formula\n",
        "        self.activation = activation #The f function in the formula\n",
        "        \n",
        "    # called the first time call is called\n",
        "    def build(self, input_shape):\n",
        "        #The initialisation parameters\n",
        "        self.mean = 0.0 \n",
        "        self.stddev = 1.0\n",
        "        dtype = 'float32'\n",
        "        self.seed = 1\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "        \n",
        "        #Initialise the variables to be trained. The variables are according to the\n",
        "        #function defined.\n",
        "        self.W = K.variable(K.random_normal((k,d,d), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.V = K.variable(K.random_normal((2*d,k), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.b = K.zeros((self.input_dim,))\n",
        "        \n",
        "        #Set the variables to be trained.\n",
        "        self._trainable_weights = [self.W, self.V, self.b]\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        #Get Both the inputs\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "\n",
        "        tf.print(\"\\n\")\n",
        "        tf.print(\"e1.shape : \", tf.shape(e1))\n",
        "        tf.print(\"e2.shape : \", tf.shape(e2))\n",
        "        tf.print(\"W[0].shape : \", tf.shape(self.W[0]))\n",
        "        tf.print(\"V.shape : \", tf.shape(self.V))\n",
        "        tf.print(\"b.shape : \", tf.shape(self.b))\n",
        "\n",
        "        #Get the batch size\n",
        "        batch_size = K.shape(e1)[0]\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "\n",
        "        #The first term in the function which is the bilinear product is calculated here.\n",
        "        first_term_k = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        tf.print(\"K.dot : \" , K.dot(e1,self.W[0]))\n",
        "        tf.print(\"K.dot shape : \" , tf.shape(K.dot(e1,self.W[0])))\n",
        "        for i in range(1, k):\n",
        "            temp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
        "            first_term_k.append(temp)\n",
        "        first_term = K.reshape(K.concatenate(first_term_k, axis=0), (batch_size, k))\n",
        "\n",
        "        #The second term in the function is calculated here.\n",
        "        second_term = K.dot(K.concatenate([e1,e2]), self.V)\n",
        "        \n",
        "        #Sum of the two terms to get the final function\n",
        "        z =  first_term + second_term\n",
        " \n",
        "        # The activation is selected here\n",
        "        if (self.activation == None):\n",
        "            return z\n",
        "        elif (self.activation == 'tanh'):\n",
        "            return K.tanh(z)\n",
        "        elif (self.activation == 'relu'):\n",
        "            return K.relu(z)\n",
        "        else :\n",
        "            print ('Activation not found')"
      ],
      "metadata": {
        "id": "7c9WtJoGtl1b"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here Define the model\n",
        "vector1 = Input(shape=(10,), dtype='float32')\n",
        "vector2 = Input(shape=(10,), dtype='float32')\n",
        "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=10, \n",
        "                                  activation= 'relu')([vector1, vector2])\n",
        "\n",
        "g = Dense(1)(BilinearLayer)\n",
        "\n",
        "#The g or the output of the modelled function.\n",
        "model = Model(inputs=[vector1, vector2], outputs=[g])\n",
        "\n",
        "#Compile the model\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "#The summary of the model.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssnK68owuovX",
        "outputId": "c432e94a-d82c-4dfd-98c9-09ca63e2f5e2"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " neural_tensor_layer_14 (Neural  (None, 32)          3850        ['input_29[0][0]',               \n",
            " TensorLayer)                                                     'input_30[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 1)            33          ['neural_tensor_layer_14[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,883\n",
            "Trainable params: 3,883\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "SHOYAvffxwwl",
        "outputId": "eb9f7c98-f70a-416a-bc49-c900a43d730c"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ll = []\n",
        "i = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "ll.append(i)\n",
        "for k in range(0,10):\n",
        "    ll.append(np.zeros(i.shape))\n",
        "\n",
        "print(ll)\n",
        "\n",
        "\n",
        "l = []\n",
        "\n",
        "j = np.array([11,12,13,14,15,16,17,18,19,20])\n",
        "l.append(j)\n",
        "for z in range(0,10):\n",
        "    l.append(np.ones(j.shape))\n",
        "\n",
        "print(l)\n",
        "\n",
        "print(np.array(l).shape)\n",
        "print(np.array(ll).shape)\n",
        "\n",
        "y = np.random.random((len(l),1))\n",
        "y.shape"
      ],
      "metadata": {
        "id": "k_8bE3iA_q8U",
        "outputId": "0ff074ac-30a6-4798-b474-5d39908fa4be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "[array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
            "(11, 10)\n",
            "(11, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(([np.array(ll), np.array(l)]), y, batch_size=4, epochs=10)"
      ],
      "metadata": {
        "id": "gpLKGofgrq29",
        "outputId": "a4cee90e-b28a-47ba-e29c-550cfc59ccae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 5s - loss: 2.1644\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 3s 34ms/step - loss: 91045.3672\n",
            "Epoch 2/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5792\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 66136.1562\n",
            "Epoch 3/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9812\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 53539.5586\n",
            "Epoch 4/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4248\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 39055.0977\n",
            "Epoch 5/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3297\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 48921.0977\n",
            "Epoch 6/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1178\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 59866.5625\n",
            "Epoch 7/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 101529.4297\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 36920.4844\n",
            "Epoch 8/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7138\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 42118.3359\n",
            "Epoch 9/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7322\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 39809.5469\n",
            "Epoch 10/10\n",
            "\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7523\n",
            "\n",
            "e1.shape :  [4 10]\n",
            "e2.shape :  [4 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "K.dot shape :  [4 10]\n",
            "\n",
            "\n",
            "e1.shape :  [3 10]\n",
            "e2.shape :  [3 10]\n",
            "W[0].shape :  [10 10]\n",
            "V.shape :  [20 32]\n",
            "b.shape :  [10]\n",
            "K.dot :  [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [-23.5958939 2.46462226 -11.866703 ... -8.75178432 1.76259947 20.083498]]\n",
            "K.dot shape :  [3 10]\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 35962.4375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f408f80e910>"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxiWrtEXuoJ7",
        "outputId": "396461a3-d7cf-432e-942e-48f5fe80997a"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here Define the model\n",
        "vector1 = Input(shape=(300,), dtype='float32')\n",
        "vector2 = Input(shape=(300,), dtype='float32')\n",
        "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=10, \n",
        "                                  activation= 'relu')([vector1, vector2])\n",
        "\n",
        "g = Dense(1)(BilinearLayer)\n",
        "\n",
        "#The g or the output of the modelled function.\n",
        "model = Model(inputs=[vector1, vector2], outputs=[g])\n",
        "\n",
        "#Compile the model\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "#The summary of the model.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe6885b-fad6-4153-b574-819047eb8008",
        "id": "6r8LSnNwTlm4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " neural_tensor_layer_9 (NeuralT  (None, 32)          3850        ['input_19[0][0]',               \n",
            " ensorLayer)                                                      'input_20[0][0]']               \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1)            33          ['neural_tensor_layer_9[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,883\n",
            "Trainable params: 3,883\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([x_train1, x_train2], y_train,\n",
        "          batch_size=64, epochs=10,\n",
        "          validation_data=([x_val1, x_val2], y_val))"
      ],
      "metadata": {
        "id": "8uGv2VC0uz_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}