{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/src/MyModel/NTN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NTN (Neural Tensor Network) [[reference_paper](https://proceedings.neurips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf)]\n",
        "\n",
        "<u><i>Goal</i></u> : state if two entities $(e_1, e_2)$ are in a certain relationship $R$.   \n",
        ">Ex. defines whehter $$(e_1, R, e_2) = (\\text{Bengal tiger}, \\text{has part}, \\text{tail})$$ is true and with which certainty.\n",
        "\n",
        "- $e_1$ and $e_2$ are vector representations or features of the two entities.\n",
        "- NTN, unlike a linear canoncical NN layer, uses a bilinear tensor layer that directly relates two entity vectors across differet dimensions.\n",
        "- Model computes a score of how likely it is two entities are in a specific position following : $$g(e_1, R, e_2) = u_R^Tf\\biggr(e_i^T W_R^{[1:K]}e_2+V_R \\begin{align}\n",
        "    \\begin{bmatrix}\n",
        "           e_{1} \\\\\n",
        "           e_{2} \\\\\n",
        "         \\end{bmatrix}\n",
        "  \\end{align} + b_R\\Biggl)$$  \n",
        "where : \n",
        "- $f=\\tanh$\n",
        "- $W_R^{[1:K]} \\in \\mathbb{R}^{d\\times d\\times k}$ is a multi-dimensional tensor\n",
        "- $e_1^TW_R^{[1:k]}e_2=h\\in\\mathbb{R}$ is the bilinear tensor\n",
        "- $V_R \\in \\mathbb{R}^{k\\times2d}$, $U \\in \\mathbb{R}^K$, $b_R\\in \\mathbb{R}^K$ are NN parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KuB3DKrgQfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### pytorch trials"
      ],
      "metadata": {
        "id": "Tq-h_R_lEKwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4WlxO-pC-Pmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorNetwork(nn.Module):\n",
        "    def __init__(self, output_dim: int, input_dim: int, activation: str=\"tanh\", mean: float=0.0, std: float=1.0):\n",
        "        \n",
        "        super(NeuralTensorNetwork, self).__init__()\n",
        "\n",
        "        # setting input and output dimensions\n",
        "        self.k = output_dim\n",
        "        self.d = input_dim # e1,e2\n",
        "\n",
        "        # setting mean and std for random initialization\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        # parameters has been used in order to consider W, V, b as model parameters\n",
        "        # inference -> they'll be optimized\n",
        "\n",
        "        # normal sampling -> https://pytorch.org/docs/stable/generated/torch.normal.html\n",
        "        # parameter -> https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
        "        self.W = torch.normal(self.mean, self.std, size=(self.k, self.d, self.d))\n",
        "        self.V = torch.normal(self.mean, self.std, size=(2*self.d, self.k))\n",
        "        self.b = torch.zeros(size=(self.d,))\n",
        "        \n",
        "        print(self.W)\n",
        "        print(self.V)\n",
        "        print(self.b)\n",
        "        \n",
        "        if activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif self.activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        # checking for a good activation function\n",
        "        else:\n",
        "            raise ValueError('Possible activation choices are tanh, sigmoid or ReLU')\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n",
        "\n",
        "        # getting the entities\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "\n",
        "        # input tensor should be of shape (batch_size, padded_length, 768)\n",
        "        batch_size = e1[0]\n",
        "        k = self.k\n",
        "        d = self.d\n",
        "\n",
        "        # bilinear tensor + bias\n",
        "        bil_bias = [torch.sum((e2 * torch.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        for i in range(1,k):\n",
        "            bil_bias.append(torch.sum((e2*torch.dot(e1, self.W[i]))) + self.b, axis=1)\n",
        "        bil_bias = torch.reshape(torch.cat(bil_bias, axis=0), (batch_size, k))\n",
        "\n",
        "        # Vr * [e1, e2]\n",
        "        rest = torch.dot(torch.cat([e1,e2]), self.V)\n",
        "\n",
        "        e1_R_e2 = bil_bias + rest\n",
        "\n",
        "        # applying activation\n",
        "        f = self.activation(e1_R_e2)\n",
        "        return f"
      ],
      "metadata": {
        "id": "-anTmLwuTLvR"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batching_data(dataset, batch_size: int=64) -> torch.utils.data.DataLoader:\n",
        "    dataloader = DataLoader(dataset=dataset, sampler=RandomSampler(dataset), batch_size=batch_size, shuffle=False)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "FtBwT8k8z-89"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = []\n",
        "i = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "ll.append(i)\n",
        "for k in range(0,10):\n",
        "    ll.append(np.zeros(i.shape))\n",
        "\n",
        "print(ll)\n",
        "\n",
        "\n",
        "l = []\n",
        "\n",
        "j = np.array([11,12,13,14,15,16,17,18,19,20])\n",
        "l.append(j)\n",
        "for z in range(0,10):\n",
        "    l.append(np.ones(j.shape))\n",
        "\n",
        "print(l)\n",
        "\n",
        "print(np.array(l).shape)\n",
        "print(np.array(ll).shape)\n",
        "\n",
        "y = np.random.random((len(l),1))\n",
        "y.shape"
      ],
      "metadata": {
        "outputId": "f48123f5-5fe5-486d-9d3e-2e4d31e5f6d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qxlLJz7YbBm"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "[array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
            "(11, 10)\n",
            "(11, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dataloader = batching_data(np.array(ll), batch_size=4)\n",
        "e2_dataloader = batching_data(np.array(l), batch_size=4)"
      ],
      "metadata": {
        "id": "jVIm2yo9gzLB"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dataloader"
      ],
      "metadata": {
        "id": "BABMrSGzhU_t",
        "outputId": "ba96d285-ad61-4755-f7e9-4fc602913011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fdaee8a0650>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e2_dataloader"
      ],
      "metadata": {
        "id": "oc9Gy7bdjfKe",
        "outputId": "950543fe-0bb8-4e54-c9f2-b6fb63167ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fdaee8a0b50>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, i in enumerate(iter(e2_dataloader)):\n",
        "    for element in i :\n",
        "        print(idx, element)"
      ],
      "metadata": {
        "id": "TL4SNMYbOjJd",
        "outputId": "928549fe-637b-4d00-9f58-0c6391bf7e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([11., 12., 13., 14., 15., 16., 17., 18., 19., 20.], dtype=torch.float64)\n",
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "0 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "1 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "2 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "2 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "2 tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(e1_dataloader))\n",
        "\n",
        "for i in range (len(iter(e1_dataloader))):\n",
        "  print(next(iter(e1_dataloader)))"
      ],
      "metadata": {
        "id": "_6N1FEmri_4R",
        "outputId": "94a44c60-48b9-4c7d-fb6b-74a211dffc71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(e2_dataloader))\n",
        "\n",
        "for i in range (len(iter(e2_dataloader))):\n",
        "  print(next(iter(e2_dataloader)))"
      ],
      "metadata": {
        "id": "cNI4NePvhf1t",
        "outputId": "5f58ef16-16d9-4ed3-ffd0-cd1b304248bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mse() -> torch.nn.MSELoss:\n",
        "    return nn.MSELoss()"
      ],
      "metadata": {
        "id": "MoLLwKrUpo2E"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr):\n",
        "    return torch.optim.Adam(model.parameters(), lr)"
      ],
      "metadata": {
        "id": "Kygj0APpqNMy"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, e1_dataloader, e2_dataloader, device='cuda'):\n",
        "    # converting the two dataloaders in iterators\n",
        "    print('training')\n",
        "    e1_iterator = iter(e1_dataloader)\n",
        "    e2_iterator = iter(e2_dataloader)\n",
        "\n",
        "    # getting lengths \n",
        "    iter1_length = len(e1_iterator)\n",
        "    iter2_length = len(e2_iterator)\n",
        "\n",
        "    flag = 0\n",
        "    loss = get_mse()\n",
        "    optimizer = get_optimizer(model, 0.001)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if iter1_length > iter2_length:\n",
        "        flag = 1\n",
        "    elif iter2_length > iter1_length:\n",
        "        flag = 2\n",
        "\n",
        "    if flag==1:\n",
        "        for i in range(iter2_length):\n",
        "        # if the second entity's vector is longer than the first one\n",
        "        # I need to deal with this case\n",
        "        # With the devised solution, once the first iterator has reached the end \n",
        "        # it will be reinitialized \n",
        "            try:\n",
        "                e1_input = next(e1_iterator)\n",
        "                e1_input = e1_input.to(device)\n",
        "            except:\n",
        "                e1_iterator = iter(e1_dataloader)\n",
        "                e1_input = next(e1_iterator)\n",
        "                e1_input = e1_input.to(device)\n",
        "\n",
        "            # at this point, al inputs are loaded on GPU\n",
        "            outputs = model([e1_input, e2_input]) # call method\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print(f'outputs: {outputs}')\n",
        "\n",
        "    elif flag==2:\n",
        "        for i in range(iter1_length):\n",
        "            try:\n",
        "                e2_input = next(e2_iterator)\n",
        "                e2_input = e2_input.to(device)\n",
        "            except:\n",
        "                e2_iterator = iter(e2_dataloader)\n",
        "                e2_input = next(e2_iterator)\n",
        "                e2_input = e2_input.to(device)\n",
        "\n",
        "            outputs = model([e1_input, e2_input]) # call method\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    elif flag==0:\n",
        "        for e1, e2 in zip(e1_iterator, e2_iterator):\n",
        "            for element1, element2 in zip(e1,e2):\n",
        "                # in case they have the same length\n",
        "                #for i in range(iter1_length):\n",
        "                e1_input = element1.to(device)\n",
        "                e2_input = element2.to(device)\n",
        "                print(e1_input)\n",
        "                print(e2_input)\n",
        "                outputs = model([e1_input, e2_input]) # call method\n",
        "        \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "3yhW3NFHjsnr"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(dataloaders, epochs=5, device='cuda'):\n",
        "    model = NeuralTensorNetwork(output_dim=32, input_dim=10).to(device)\n",
        "    for e in range(epochs):\n",
        "    # def training_step(model, e1_dataloader, e2_dataloader, device='cuda'):\n",
        "        training_step(model, dataloaders[0], dataloaders[1]) \n",
        "        print(e)\n",
        "    print('Traning done')"
      ],
      "metadata": {
        "id": "8_HKAPHSHOsx"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main([e1_dataloader, e2_dataloader])"
      ],
      "metadata": {
        "id": "MUzSiL92JgIP",
        "outputId": "0d398173-2405-43bb-c439-a8485d6a3ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.7292,  1.6683, -1.3696,  ...,  0.2507,  0.2852,  0.8378],\n",
            "         [-0.8256,  0.6604, -2.3937,  ..., -1.4956,  2.0681, -0.6468],\n",
            "         [-0.5461, -0.8905,  0.4425,  ...,  0.0828, -0.7148,  1.1008],\n",
            "         ...,\n",
            "         [ 1.5781, -1.7978, -0.6980,  ..., -0.4238,  0.5031,  0.9757],\n",
            "         [-0.3200,  0.4735,  0.1175,  ...,  1.8322,  0.4655, -0.6373],\n",
            "         [-1.0224,  1.7510,  0.2344,  ..., -1.5151,  1.9617, -0.1190]],\n",
            "\n",
            "        [[ 1.2196, -0.3441,  1.9830,  ...,  1.1283, -1.2622,  0.4010],\n",
            "         [-0.5095,  0.0877,  0.6861,  ...,  1.4973, -1.0496, -0.9951],\n",
            "         [ 1.2247,  0.7271, -0.0622,  ...,  1.4032,  0.1916, -0.6052],\n",
            "         ...,\n",
            "         [ 0.3109,  0.5520, -1.9498,  ...,  0.2184, -0.4450,  0.6122],\n",
            "         [ 0.8234, -1.2088,  1.2548,  ..., -0.5024, -0.7056, -0.0105],\n",
            "         [-0.0912,  0.3408,  0.2173,  ...,  0.4572, -0.2117, -0.8949]],\n",
            "\n",
            "        [[-0.1551, -0.9882,  0.7685,  ..., -0.6940, -0.5335,  0.2737],\n",
            "         [ 1.1150, -2.0656, -0.2730,  ...,  1.2818, -0.8269, -0.5196],\n",
            "         [-1.3238,  1.2023, -0.2132,  ...,  0.1508, -0.5040, -0.6651],\n",
            "         ...,\n",
            "         [-0.8577,  1.4975, -1.9356,  ...,  0.7055, -1.6009, -0.7188],\n",
            "         [ 0.3795,  0.1849,  1.3870,  ...,  1.4864, -0.5809,  0.6102],\n",
            "         [ 1.4740,  1.6935,  1.9960,  ..., -2.2834, -0.8161, -0.7781]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.8240, -1.0431,  1.1920,  ..., -0.2060,  0.2089, -1.3227],\n",
            "         [ 0.4799,  2.1955, -0.8923,  ..., -0.6427,  2.5241, -0.2668],\n",
            "         [-0.7416,  0.7582, -0.6167,  ...,  0.5380, -0.0279,  0.5574],\n",
            "         ...,\n",
            "         [ 0.1421,  0.5898,  0.0862,  ...,  0.0468, -0.7656,  0.2423],\n",
            "         [-1.8957,  0.4862,  1.6079,  ..., -0.9453, -0.3867,  0.1926],\n",
            "         [-0.1561,  0.1227,  0.0763,  ..., -0.8865, -0.4112, -0.2994]],\n",
            "\n",
            "        [[-1.1642, -0.1593,  0.1666,  ..., -0.0788, -1.5495,  0.7309],\n",
            "         [ 0.1849,  0.7317,  0.5574,  ...,  0.3359,  0.8143, -0.0337],\n",
            "         [-1.4845,  0.0599,  0.5446,  ..., -0.5295, -0.0916, -1.1981],\n",
            "         ...,\n",
            "         [-0.1187, -2.1762,  2.2198,  ..., -0.4423,  0.4453, -0.3305],\n",
            "         [-0.2580,  0.2228, -0.6638,  ..., -0.0891,  0.5803,  0.5084],\n",
            "         [-0.0441,  0.8578, -0.1928,  ..., -0.3523, -1.6901, -0.4004]],\n",
            "\n",
            "        [[-1.2837, -1.0049,  1.9316,  ..., -1.1506, -0.1933, -1.7451],\n",
            "         [-0.7180,  1.0602,  0.2610,  ...,  1.7627, -0.7189,  0.2987],\n",
            "         [ 0.2737, -1.1211, -0.1533,  ..., -0.3288, -0.7441, -1.8026],\n",
            "         ...,\n",
            "         [-0.1445,  1.2669,  0.1658,  ..., -0.6564, -0.8191,  1.7024],\n",
            "         [ 0.8042,  0.7663,  1.3553,  ...,  2.4624, -1.1190,  1.4512],\n",
            "         [-0.4961,  0.2361,  0.0455,  ..., -1.3167,  1.0284, -1.2652]]])\n",
            "tensor([[-1.7989e-01,  1.8716e+00, -4.0006e-01, -1.0113e+00,  1.6376e+00,\n",
            "         -7.6471e-01,  6.8879e-01,  4.1292e-01, -1.4643e-01,  5.4656e-01,\n",
            "          1.5223e-01,  1.5695e-02,  5.0016e-02, -1.0467e+00, -1.1164e+00,\n",
            "         -1.3461e+00,  2.7613e-01, -4.6693e-01, -1.0067e+00,  6.7397e-01,\n",
            "          7.6765e-01, -1.0282e+00,  4.2079e-01,  9.2580e-02,  1.6294e+00,\n",
            "          3.6841e-01, -6.8217e-02, -4.6687e-01,  2.5903e+00, -1.1781e+00,\n",
            "          8.7629e-01,  2.2408e-01],\n",
            "        [-1.3986e-01,  3.1107e-01,  5.5890e-01, -8.1049e-01,  2.1602e-01,\n",
            "         -8.2149e-01, -2.0626e+00, -4.0026e-01,  2.7647e-01, -5.7037e-01,\n",
            "          4.0682e-01,  1.3049e-01,  2.3532e+00, -8.3916e-03, -6.0199e-02,\n",
            "          2.1574e-01, -9.6431e-01, -7.4124e-01, -5.5225e-01, -6.4549e-01,\n",
            "          4.0103e-01, -1.1547e+00, -8.9974e-01,  1.4158e+00, -1.3414e+00,\n",
            "         -6.6342e-01,  6.7658e-02,  1.0860e+00, -4.5260e-01, -5.0777e-01,\n",
            "          3.5429e-01, -2.0661e+00],\n",
            "        [ 1.0370e+00, -1.1205e+00, -4.1431e-01,  4.2013e-01, -3.6497e-01,\n",
            "         -2.9277e-02,  7.9238e-01,  1.5726e+00,  2.3721e-01,  6.9083e-01,\n",
            "         -4.0626e-01,  7.0090e-01, -1.1587e-01,  7.1424e-01, -8.4606e-01,\n",
            "          5.7038e-01, -4.4915e-01,  1.9288e+00, -9.5199e-01,  1.3694e+00,\n",
            "          8.5520e-01,  3.1526e-01,  3.4779e-01,  7.1172e-02, -1.5229e+00,\n",
            "          5.6882e-01, -2.6925e-01,  2.1846e+00,  1.3889e+00,  5.0509e-01,\n",
            "         -5.8625e-01,  1.1700e+00],\n",
            "        [-2.9312e-01,  7.6096e-01, -8.1302e-01, -1.2407e-01,  1.1211e+00,\n",
            "         -4.5671e-01, -1.0891e+00,  1.1807e+00, -1.4967e+00,  4.2534e-01,\n",
            "         -1.9972e+00, -1.9111e+00, -8.9249e-01,  7.7686e-01, -4.4139e-01,\n",
            "         -2.5307e-01,  5.0484e-01, -2.3460e-01,  4.4192e-01, -6.5810e-02,\n",
            "          2.4133e-01, -8.4527e-02, -1.4546e-01,  1.5027e+00,  1.1414e+00,\n",
            "         -1.0321e-01,  3.9648e-01, -1.0722e+00, -9.2117e-01, -2.8252e-01,\n",
            "          6.4487e-01, -7.9774e-01],\n",
            "        [ 1.0349e+00, -1.2385e+00, -5.3334e-01,  1.0779e+00, -3.8642e-01,\n",
            "         -1.0889e+00,  4.1243e-01,  1.2475e+00,  2.6665e-01,  9.3204e-03,\n",
            "         -4.3807e-02,  2.8513e-01, -1.3634e+00, -1.8504e-01,  3.0853e-01,\n",
            "         -1.9021e+00, -3.2607e-01,  1.5108e-01, -5.1301e-01, -5.6498e-01,\n",
            "          8.2400e-01, -5.6964e-01,  1.3860e+00, -1.4031e+00,  1.5788e+00,\n",
            "          5.1934e-02,  8.9308e-01,  4.2216e-01,  3.3787e-01,  1.1420e+00,\n",
            "          1.0271e+00,  5.5420e-01],\n",
            "        [ 1.6858e-01,  8.2411e-01,  1.4297e+00,  4.5626e-02,  9.8050e-01,\n",
            "         -4.8517e-01, -3.2599e-01,  3.5073e-01, -1.6240e+00, -2.8921e-01,\n",
            "         -2.0259e-03, -9.1194e-01,  2.1101e-01,  7.7661e-01, -7.5364e-01,\n",
            "          7.8719e-01,  3.7373e-02, -9.4110e-01,  6.8587e-01, -4.7128e-01,\n",
            "         -7.6672e-01,  5.6981e-01,  5.8994e-01, -4.4022e-01, -1.9950e-01,\n",
            "          4.3012e-01,  1.5261e-01, -4.2815e-01, -1.3177e+00,  5.7010e-01,\n",
            "          1.0726e+00,  1.6494e+00],\n",
            "        [-7.5047e-01,  1.0002e+00, -1.3158e+00, -9.1340e-02, -1.3499e+00,\n",
            "          1.2799e+00,  7.1376e-01,  3.5863e-01, -5.3389e-01,  6.8660e-01,\n",
            "         -6.5226e-01, -2.6841e+00,  1.4918e-01,  1.2574e+00,  1.5452e-01,\n",
            "          2.5688e-01, -1.3861e-01,  4.5398e-01, -5.9829e-01,  4.6392e-02,\n",
            "         -7.4607e-01,  1.1188e-01, -7.2593e-01,  1.0675e+00, -1.5333e+00,\n",
            "          1.4074e+00, -1.4981e+00,  2.3444e+00,  1.0229e+00, -3.1340e-01,\n",
            "         -6.9103e-02, -1.1336e+00],\n",
            "        [-2.9243e-01,  7.6501e-01, -8.1569e-01,  3.9442e-01,  4.3498e-01,\n",
            "         -2.0028e-01,  8.3113e-01,  1.3358e-01,  1.3617e+00,  8.0717e-02,\n",
            "         -2.3428e+00,  3.3309e-01,  1.0459e+00,  1.5141e+00,  5.3955e-01,\n",
            "          2.2074e+00, -1.1804e+00, -2.2029e+00, -8.6212e-01, -2.2742e-01,\n",
            "         -1.5085e-02, -3.5023e-01, -8.8732e-01, -4.3325e-01,  1.0067e+00,\n",
            "         -7.4208e-01, -1.3813e+00, -1.1236e-02,  2.5851e-01, -2.9468e-02,\n",
            "          1.8158e+00, -1.9049e-01],\n",
            "        [-6.8819e-01,  5.8993e-01, -1.0761e+00, -1.0758e+00,  5.1650e-01,\n",
            "         -4.9512e-01, -5.3953e-01, -6.6483e-01, -8.8489e-01,  3.8385e-01,\n",
            "          2.1789e-01, -4.6505e-01,  1.2616e+00,  6.9463e-01,  4.0189e-01,\n",
            "         -5.1425e-01, -6.3492e-01, -4.5999e-02,  4.6144e-01,  6.5482e-01,\n",
            "          5.5092e-01,  8.4475e-01, -2.3863e+00,  9.7521e-01,  2.4514e-01,\n",
            "         -2.6302e-01, -5.1682e-01,  6.3405e-01, -1.0218e+00,  3.8715e-01,\n",
            "          2.7436e+00, -8.5293e-01],\n",
            "        [-5.5924e-01,  1.1882e+00,  9.6207e-01, -5.2480e-01, -2.6777e-01,\n",
            "          1.0506e+00, -1.1454e+00, -1.2474e+00,  7.8841e-01,  7.8778e-01,\n",
            "          8.4464e-03, -1.6219e+00,  7.5532e-01,  1.3473e+00, -5.7594e-01,\n",
            "         -8.6966e-01, -7.9572e-01, -1.2195e+00,  1.4717e+00,  1.6771e+00,\n",
            "         -1.9854e-01, -1.2187e+00, -7.9607e-01,  3.5451e-01,  5.4447e-01,\n",
            "         -6.0073e-01,  2.4799e-01,  5.3896e-02,  1.4888e-01,  4.4880e-01,\n",
            "         -6.2433e-01,  1.4030e+00],\n",
            "        [ 9.3374e-01, -2.8728e+00, -1.1158e+00, -9.7403e-01, -6.4487e-01,\n",
            "          8.3627e-01, -5.3792e-01, -7.5361e-01, -3.8977e-01, -1.3859e+00,\n",
            "          3.9189e-01, -2.3352e-01,  7.4086e-01, -7.5536e-01,  6.4952e-01,\n",
            "         -4.1487e-01, -4.5486e-02, -7.3784e-01, -5.9142e-01,  9.0481e-01,\n",
            "         -4.4921e-01, -9.1044e-01,  4.6191e-01, -4.7571e-01,  7.0192e-01,\n",
            "          1.1158e+00, -1.8671e+00, -2.8605e-01,  1.0688e+00,  7.2160e-01,\n",
            "          2.9652e-01,  8.4893e-01],\n",
            "        [ 4.7472e-02,  5.1680e-02, -4.4675e-01, -6.8275e-01, -2.0174e-01,\n",
            "          3.3585e-01,  6.3000e-01,  3.0776e-02,  1.4460e+00, -3.0307e-01,\n",
            "          5.0944e-01, -4.7035e-01,  5.9575e-01, -1.2325e+00,  9.7252e-02,\n",
            "         -3.3813e-01,  5.4686e-02,  1.6500e+00, -3.7852e-01,  3.3802e-01,\n",
            "         -5.7277e-01, -1.1057e+00,  1.3163e+00, -7.3495e-01, -6.6185e-01,\n",
            "         -8.1306e-01, -9.1987e-01,  2.8173e-01, -1.2783e+00,  1.3291e+00,\n",
            "         -1.4550e+00,  7.3165e-01],\n",
            "        [-9.7154e-01, -1.6359e+00,  9.4221e-01, -1.4491e-01, -1.1983e-01,\n",
            "          9.2022e-01, -2.7359e-02,  3.6630e-01, -4.8554e-01, -8.0316e-01,\n",
            "         -2.2673e-01,  3.7346e-01,  1.0623e-01, -2.5849e-01, -5.0906e-01,\n",
            "          7.3950e-01, -1.9346e+00,  4.2583e-01, -1.1060e+00,  4.5962e-02,\n",
            "         -9.6074e-01, -1.5865e+00, -4.6064e-01, -4.7331e-01,  1.7528e+00,\n",
            "          3.0755e-01,  6.9781e-02,  7.4996e-01,  2.1470e+00, -1.0184e+00,\n",
            "         -5.3734e-03,  4.9533e-01],\n",
            "        [ 6.0952e-01, -2.1142e+00, -5.9954e-01, -1.1516e+00, -1.1503e+00,\n",
            "          8.2138e-01, -7.0189e-01, -3.4173e-01, -7.6754e-02,  2.8168e-01,\n",
            "          4.5083e-01,  2.0761e-01, -9.7920e-02,  1.5797e+00, -8.5622e-01,\n",
            "         -3.3945e-01,  1.0873e+00,  8.6523e-01,  5.2452e-01, -1.5174e+00,\n",
            "         -3.5632e-01,  4.7529e-01,  2.6541e-01,  1.3081e+00,  1.6278e+00,\n",
            "          1.0324e+00,  1.4922e+00, -6.7860e-01, -4.8054e-01, -7.1400e-01,\n",
            "          1.6394e+00,  8.2248e-01],\n",
            "        [ 2.1115e+00, -1.2147e+00,  1.1263e+00,  1.4424e+00, -5.4954e-01,\n",
            "         -1.2030e+00,  2.4332e-02, -3.6209e-01,  1.1987e+00, -1.0660e-01,\n",
            "          6.3510e-02, -1.5845e+00, -1.6835e+00,  7.1773e-01, -9.2370e-01,\n",
            "         -4.7162e-01, -1.0618e+00, -5.4557e-01,  2.8753e-01,  1.4942e-01,\n",
            "          7.7286e-01,  1.4474e+00,  1.8076e+00,  2.1563e-01, -1.9000e+00,\n",
            "          6.9543e-01,  1.1208e+00, -4.4000e-01,  4.6650e-01, -5.4550e-02,\n",
            "         -1.3042e+00,  9.8532e-01],\n",
            "        [-2.2386e-01,  1.0992e+00, -9.2399e-02, -7.9973e-01, -2.0923e-01,\n",
            "         -3.5939e-01, -5.6095e-03,  2.4239e-01,  2.2738e-01,  7.0730e-01,\n",
            "          3.4586e-01, -2.6112e-01, -4.0662e-02,  1.2257e+00,  1.5573e+00,\n",
            "          8.6628e-01, -5.7996e-01, -3.9290e-01,  1.1440e+00, -5.4483e-01,\n",
            "         -2.2309e-01, -3.6237e-01,  5.4700e-01,  4.6651e-01, -8.4778e-01,\n",
            "         -1.1813e+00,  5.9779e-01,  9.6791e-02,  8.9634e-01,  1.0204e+00,\n",
            "          3.3312e-01,  4.7575e-01],\n",
            "        [ 4.4941e-01, -1.2155e+00,  5.0037e-01, -2.0645e+00,  1.8184e-01,\n",
            "         -3.3109e-01, -1.5103e+00,  3.6192e-01,  2.2636e-01,  8.6572e-01,\n",
            "          2.2510e+00,  1.2756e+00, -1.7539e+00,  4.2933e-01,  8.9488e-01,\n",
            "         -7.8265e-01, -3.6721e-01,  1.1726e-01, -1.5085e-01, -6.6455e-02,\n",
            "         -4.0694e-01,  7.7677e-01, -8.1641e-01,  1.4369e+00,  5.7002e-01,\n",
            "         -4.1376e-01,  1.0489e+00,  9.9616e-01, -8.7877e-01,  1.5180e+00,\n",
            "         -9.6648e-01, -5.2219e-01],\n",
            "        [ 1.0486e+00,  1.3405e+00,  2.0588e-01, -9.8890e-01,  3.1030e-02,\n",
            "          6.7024e-01, -6.1455e-01, -1.4773e+00, -5.9750e-01, -3.6239e-01,\n",
            "         -1.9338e+00, -8.5538e-01, -8.3265e-01, -1.0931e+00, -5.3216e-01,\n",
            "         -1.4637e+00, -9.0539e-01,  2.1365e+00,  2.9506e-01,  4.0999e-01,\n",
            "          8.8215e-01,  1.4077e-01, -2.1873e+00, -1.0856e-01, -9.3389e-01,\n",
            "         -2.8841e+00, -1.1664e-01, -6.6310e-01,  1.7436e-01,  3.2016e-01,\n",
            "         -8.5739e-01, -1.2735e+00],\n",
            "        [ 7.7181e-01,  6.5973e-01, -5.0507e-01, -4.2102e-01, -9.5530e-01,\n",
            "         -1.2132e+00, -3.7676e-01, -5.2040e-01, -8.7760e-01,  8.9662e-01,\n",
            "          1.5650e+00,  7.4089e-01, -1.1595e+00,  2.3231e+00, -1.9566e+00,\n",
            "         -1.1958e-01,  1.1119e+00, -3.1461e-01, -3.9240e-01, -2.1562e+00,\n",
            "          1.6884e-01, -3.6157e-02,  1.7106e+00,  1.4546e+00,  2.6239e-01,\n",
            "         -1.3359e+00, -1.6720e+00,  2.9818e-01,  5.3742e-01,  2.6574e-02,\n",
            "          1.3130e+00, -1.7058e-01],\n",
            "        [-7.1480e-02, -3.8215e-01,  1.5043e-01, -2.5319e-01,  1.8122e-01,\n",
            "         -1.1581e+00, -7.4644e-02,  1.7891e-02,  6.8335e-01, -1.1324e+00,\n",
            "          9.5067e-01, -4.4448e-01,  3.5535e-01,  4.4942e-01, -8.5254e-02,\n",
            "         -1.3283e+00, -4.5202e-01, -2.8445e-01,  7.9360e-01, -1.9381e+00,\n",
            "          8.9888e-01,  3.5156e-01, -1.4235e+00,  8.4497e-01, -8.2378e-02,\n",
            "         -2.9396e-01,  3.9155e-01,  1.8191e+00,  5.3373e-01, -1.0999e+00,\n",
            "         -5.3545e-01, -1.7771e-01]])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "training\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-fbc7d7558f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2_dataloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-125-7eb79aa028e9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataloaders, epochs, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# def training_step(model, e1_dataloader, e2_dataloader, device='cuda'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Traning done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-b1e9e8a78406>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, e1_dataloader, e2_dataloader, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-0720af2edd5b>\u001b[0m in \u001b[0;36mget_optimizer\u001b[0;34m(model, lr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable)\u001b[0m\n\u001b[1;32m     88\u001b[0m                         \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         maximize=maximize, foreach=foreach, capturable=capturable)\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### to collapse"
      ],
      "metadata": {
        "id": "bfw_Ard1YVR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "TXq4IXC-pYRW",
        "outputId": "3c92b12f-ca50-45f5-8aad-36884859581a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = batching_data([x_train1, x_train2])"
      ],
      "metadata": {
        "id": "7xvVymYO2uAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlE35uqf7ItV",
        "outputId": "d207aaa8-0855-460b-a696-d518aa71506a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FgpHwh97N1N",
        "outputId": "a3c33fdc-9f83-4248-9e3e-aa8725c1992b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensrflow Version"
      ],
      "metadata": {
        "id": "XYzKkf7VEREB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K  # in keras simple computations are not handled directly but it relies on a well optimized tensor handler library\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "2zWIRcpstrr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorLayer(Layer):\n",
        "    def __init__(self, output_dim, input_dim, activation= None):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim #The k in the formula\n",
        "        self.input_dim = input_dim   #The d in the formula\n",
        "        self.activation = activation #The f function in the formula\n",
        "        \n",
        "    # called the first time call is called\n",
        "    def build(self, input_shape):\n",
        "        #The initialisation parameters\n",
        "        self.mean = 0.0 \n",
        "        self.stddev = 1.0\n",
        "        dtype = 'float32'\n",
        "        self.seed = 1\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "        \n",
        "        #Initialise the variables to be trained. The variables are according to the\n",
        "        #function defined.\n",
        "        self.W = K.variable(K.random_normal((k,d,d), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.V = K.variable(K.random_normal((2*d,k), self.mean, self.stddev,\n",
        "                               dtype=dtype, seed=self.seed))\n",
        "        self.b = K.zeros((self.input_dim,))\n",
        "        \n",
        "        #Set the variables to be trained.\n",
        "        self._trainable_weights = [self.W, self.V, self.b]\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        #Get Both the inputs\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "\n",
        "        tf.print(e1)\n",
        "        tf.print(e2)\n",
        "\n",
        "        #Get the batch size\n",
        "        batch_size = K.shape(e1)[0]\n",
        "        \n",
        "        #The output and the inut dimension\n",
        "        k = self.output_dim\n",
        "        d = self.input_dim\n",
        "\n",
        "        #The first term in the function which is the bilinear product is calculated here.\n",
        "        first_term_k = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
        "        for i in range(1, k):\n",
        "            temp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
        "            first_term_k.append(temp)\n",
        "        first_term = K.reshape(K.concatenate(first_term_k, axis=0), (batch_size, k))\n",
        "\n",
        "        #The second term in the function is calculated here.\n",
        "        second_term = K.dot(K.concatenate([e1,e2]), self.V)\n",
        "        \n",
        "        #Sum of the two terms to get the final function\n",
        "        z =  first_term + second_term\n",
        " \n",
        "        # The activation is selected here\n",
        "        if (self.activation == None):\n",
        "            return z\n",
        "        elif (self.activation == 'tanh'):\n",
        "            return K.tanh(z)\n",
        "        elif (self.activation == 'relu'):\n",
        "            return K.relu(z)\n",
        "        else :\n",
        "            print ('Activation not found')"
      ],
      "metadata": {
        "id": "7c9WtJoGtl1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "x_train1 = np.random.random((1000, 300))\n",
        "x_train2 = np.random.random((1000, 300))\n",
        "y_train = np.random.random((1000, 1))\n",
        "\n",
        "# Dummy validation data\n",
        "x_val1 = np.random.random((100, 300))\n",
        "x_val2 = np.random.random((100, 300))\n",
        "y_val = np.random.random((100, 1))\n",
        "\n",
        "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
        "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxiWrtEXuoJ7",
        "outputId": "c1cca00d-0307-48ab-f180-3a7c4d0599c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
            "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here Define the model\n",
        "vector1 = Input(shape=(10,), dtype='float32')\n",
        "vector2 = Input(shape=(10,), dtype='float32')\n",
        "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=10, \n",
        "                                  activation= 'relu')([vector1, vector2])\n",
        "\n",
        "g = Dense(1)(BilinearLayer)\n",
        "\n",
        "#The g or the output of the modelled function.\n",
        "model = Model(inputs=[vector1, vector2], outputs=[g])\n",
        "\n",
        "#Compile the model\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "#The summary of the model.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssnK68owuovX",
        "outputId": "22d00b79-4cab-421c-87df-5a659b48453c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " neural_tensor_layer_17 (Neural  (None, 32)          3850        ['input_35[0][0]',               \n",
            " TensorLayer)                                                     'input_36[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 1)            33          ['neural_tensor_layer_17[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,883\n",
            "Trainable params: 3,883\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([x_train1, x_train2], y_train,\n",
        "          batch_size=64, epochs=10,\n",
        "          validation_data=([x_val1, x_val2], y_val))"
      ],
      "metadata": {
        "id": "8uGv2VC0uz_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "SHOYAvffxwwl",
        "outputId": "48caa036-93eb-484a-bb86-a852b5801a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ll = []\n",
        "i = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "ll.append(i)\n",
        "for k in range(0,10):\n",
        "    ll.append(np.zeros(i.shape))\n",
        "\n",
        "print(ll)\n",
        "\n",
        "\n",
        "l = []\n",
        "\n",
        "j = np.array([11,12,13,14,15,16,17,18,19,20])\n",
        "l.append(j)\n",
        "for z in range(0,10):\n",
        "    l.append(np.ones(j.shape))\n",
        "\n",
        "print(l)\n",
        "\n",
        "print(np.array(l).shape)\n",
        "print(np.array(ll).shape)\n",
        "\n",
        "y = np.random.random((len(l),1))\n",
        "y.shape"
      ],
      "metadata": {
        "id": "k_8bE3iA_q8U",
        "outputId": "85489be2-757d-45b4-ca17-36d5ee4e777c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "[array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
            "(11, 10)\n",
            "(11, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(([np.array(ll), np.array(l)]), y, batch_size=4, epochs=10)"
      ],
      "metadata": {
        "id": "gpLKGofgrq29",
        "outputId": "36ebea57-86e7-40a6-a9f0-35ff27c0582c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 24.9693[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 55296.9531\n",
            "Epoch 2/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 22.0459[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 52723.6250\n",
            "Epoch 3/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 24.0815[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 49257.1250\n",
            "Epoch 4/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 232299.7969[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 84486.7422\n",
            "Epoch 5/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 23.1153[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 29888.4434\n",
            "Epoch 6/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 20.3125[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 79863.0469\n",
            "Epoch 7/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 214036.5938[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 77845.0703\n",
            "Epoch 8/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 23.1210[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 37405.0781\n",
            "Epoch 9/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 21.8295[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70559.7266\n",
            "Epoch 10/10\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 21.3914[[0 0 0 ... 0 0 0]\n",
            " [1 2 3 ... 8 9 10]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [11 12 13 ... 18 19 20]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 33952.4648\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0581110f50>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_Bll6SNu12J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}