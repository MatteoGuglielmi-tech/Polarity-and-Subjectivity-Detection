{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/src/MyModel/BERT-Fine-Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Embedding Fine Tuning"
      ],
      "metadata": {
        "id": "gxa6J-lH4jrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Pu3CpB90ga",
        "outputId": "ee30ce33-9903-4881-b2e5-0d818adb12e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFjx3Xf38J7l",
        "outputId": "07fc1383-d253-4c19-fa65-270dae2a1da2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rootdir = '/content/gdrive/MyDrive/Colab Notebooks/Polarity-Subjectivity-Detection/'"
      ],
      "metadata": {
        "id": "suw-3tTbAfrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name\n",
        "device = tf.test.gpu_device_name()\n",
        "\n",
        "if 'GPU' in device:\n",
        "  print(f'GPU available : {device}')\n",
        "  device = torch.device('cuda'+device[-2:])\n",
        "else :\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU not found, CPU will be instead\")\n",
        "  #raise SystemError(\"GPU not found, use CPU instead\")"
      ],
      "metadata": {
        "id": "Tl20NCMY6_xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "p6buUIlod0K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# loading datasets\n",
        "movie_reviews = pd.read_csv(rootdir+'Datasets/movie_rews.csv')                  # with labels\n",
        "movie_rews_clean = pd.read_csv(rootdir+'Datasets/movie_rews_clean.csv')         # without labels\n",
        "movie_rews_raw = pd.read_csv(rootdir+'Datasets/movie_rews_raw.csv')             # without labels and pre-processing\n",
        "\n",
        "subj_obj = pd.read_csv(rootdir+'Datasets/subj_obj_dataset.csv')                 # with labels\n",
        "subj_obj_clean = pd.read_csv(rootdir+'Datasets/subj_obj_dataset_clean.csv')     # without labels"
      ],
      "metadata": {
        "id": "TZCUmC4h7WEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews"
      ],
      "metadata": {
        "id": "mDrScMj7Asm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_rews_clean"
      ],
      "metadata": {
        "id": "apPHZW_ZHzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_rews_raw"
      ],
      "metadata": {
        "id": "RAz-HXLVH2Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subj_obj"
      ],
      "metadata": {
        "id": "DujGO_7-AwXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subj_obj_clean"
      ],
      "metadata": {
        "id": "XvkRPPRPH_TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subjective = subj_obj_dataset[['text','labels']][:len(subj_obj_dataset)//2].sample(n=1000)"
      ],
      "metadata": {
        "id": "G9UOFOzx1_cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#objective = subj_obj_dataset[['text', 'labels']][len(subj_obj_dataset)//2:].sample(n=1000)"
      ],
      "metadata": {
        "id": "EI6BUx7F4GiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subj_obj_dataset = pd.concat([subjective,objective], axis=0)"
      ],
      "metadata": {
        "id": "QMXa0AnE5HPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subj_obj_dataset"
      ],
      "metadata": {
        "id": "LobPS0qV5aXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subjective = subj_obj_clean[['text']][:len(subj_obj_clean)//2].sample(n=1000)\n",
        "objective = subj_obj_clean[['text']][len(subj_obj_clean)//2:].sample(n=1000)\n",
        "subj_obj_dataset = pd.concat([subjective,objective], axis=0)"
      ],
      "metadata": {
        "id": "4MuNN3oHWaVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subj_obj_dataset"
      ],
      "metadata": {
        "id": "SqWi-Gl5Wsx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Major commands :\n",
        "- .tokenize(sent)\n",
        "- .convert_tokens_to_ids(tokenized_sent)\n",
        "- .encode.plus() [source](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus)"
      ],
      "metadata": {
        "id": "RnYxmD2FEySp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT model script from: huggingface.co\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import Tuple, List, Dict\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import logging\n",
        "import gc\n",
        "\n",
        "# to not see warning everytime\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "KuyfsCFkKaSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer,'bert-base-uncased')\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "metadata": {
        "id": "vBlWgDyNrxkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(dataset : pd.DataFrame, \n",
        "              sentence_column: str, \n",
        "              tokenizer: BertTokenizer, \n",
        "              maxlen: int\n",
        "              ) -> Tuple[torch.Tensor]:  \n",
        "    '''Extract embedding information using passed tokenizer and model\n",
        "        Params:\n",
        "        ------\n",
        "            dataset : pd.DataFrame\n",
        "                dataframe containig sentences to be encoded\n",
        "            sentence_column : str\n",
        "                column in the beforementioned dataframe to be encoded\n",
        "            tokenizer : transformers.models.bert.tokenization_bert.BertTokenizer\n",
        "                tokenizer used to process sentences and extract related information\n",
        "        Return:\n",
        "        ------\n",
        "            Return encodings and attention masks as pytorch tensors\n",
        "    '''\n",
        "    \n",
        "    embeddings = {\n",
        "        'embedding' : [],\n",
        "        'attention_mask' : []\n",
        "        }\n",
        "        \n",
        "    for sent in dataset[sentence_column]:\n",
        "        dic_sent_encoding = tokenizer.encode_plus(sent,                         # untokenized sentence\n",
        "                                                add_special_tokens = True,      # add '[CLS]' and '[SEP]'\n",
        "                                                truncation = True,              # truncate to maximum length\n",
        "                                                max_length = maxlen,            # due to ram limiatation \n",
        "                                                padding = \"max_length\",         # pad to maximum admissible sentence\n",
        "                                                return_attention_mask = True,   # return attention mask\n",
        "                                                return_tensors = \"pt\")          # returns pytorch tensors\n",
        "    \n",
        "        # extracting embeddings and attention masks in list form\n",
        "        embeddings['embedding'].append(dic_sent_encoding['input_ids'])\n",
        "        embeddings['attention_mask'].append(dic_sent_encoding['attention_mask'])\n",
        "\n",
        "    # convert lists of tensors into tensors\n",
        "    #input_ids = torch.cat(embeddings['embedding'], axis=0)\n",
        "    #attention_masks = torch.cat(embeddings['attention_mask'], axis=0)\n",
        "    \n",
        "    #return input_ids, attention_masks\n",
        "    return embeddings['embedding'], embeddings['attention_mask']"
      ],
      "metadata": {
        "id": "I1Ov7l-CAyG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mr_encoding, attention_masks_mr = embedding(dataset=movie_reviews, sentence_column='text', tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "bslSQQ1VagQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to display a sample tensor uncomment following lines\n",
        "#mr_encoding[0], attention_masks_mr[0]"
      ],
      "metadata": {
        "id": "0Wa8BAy3SaQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#so_encoding, attention_masks_so = embedding(dataset=subj_obj_dataset, sentence_column='text', tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "A_RI20fmM0JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to display a sample tensor uncomment following lines\n",
        "#so_encoding[0], attention_masks_so[0]"
      ],
      "metadata": {
        "id": "QmB8kLzASrTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to investigate size of a tensor uncomment following lones\n",
        "#so_encoding[0].size(), attention_masks_so[0].size()"
      ],
      "metadata": {
        "id": "LCs25QfZTYpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BATCH_SIZE = 128 # reduced due to RAM limitations"
      ],
      "metadata": {
        "id": "uNy-pqdEnS-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "KL3poEpshKet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings(dataset: pd.DataFrame, \n",
        "               tokenizer: BertTokenizer, \n",
        "               model: BertModel, \n",
        "               maxlen: int, \n",
        "               column_name: str='text',\n",
        "               device: str=device\n",
        "               )-> List:\n",
        "    '''Function to batch sentences to fit BERT model and get embeddings\n",
        "\n",
        "        Params:\n",
        "        ------\n",
        "            dataset : pd.DataFrame\n",
        "                dataset to be batched\n",
        "            column_name : str\n",
        "                column of 'dataset' DataFrame to catch \n",
        "        \n",
        "        Return:\n",
        "        ------\n",
        "            tuple of dataloader to iterate over\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    embs = []\n",
        "    # getting encodings and attention masks for whole dataset\n",
        "    emb, msk = embedding(dataset=dataset, sentence_column=column_name, tokenizer=tokenizer, maxlen=maxlen)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sen, e in enumerate(zip(emb,msk)):\n",
        "            #print(f\"Sanity check emb!=msk : {e[0]==e[1]}\", end='\\r')\n",
        "            last_hidden_states = model(e[0].to(device),e[1].to(device))\n",
        "            sys.stdout.write('\\r'+\"Sentence {} of shape {}\".format(sen, last_hidden_states[0].shape))\n",
        "            sys.stdout.flush()\n",
        "            #print(\"Sentence {} of shape {}\".format(sen, last_hidden_states[0].shape), end='\\x1b[1K\\r')\n",
        "            embs.append(last_hidden_states[0])\n",
        "\n",
        "    return embs"
      ],
      "metadata": {
        "id": "16S6YJ7wnBZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "def clear_folder_content(folder):\n",
        "    for filename in os.listdir(folder):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
      ],
      "metadata": {
        "id": "0gv_QzUzQxP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_folder_content(rootdir+'BERT/Polarity-Embeddings')\n",
        "clear_folder_content(rootdir+'BERT/Subjectivity-Embeddings')"
      ],
      "metadata": {
        "id": "LkJR3L-WbJk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_folder_content(rootdir+'BERT/Polarity-Embeddings-Truncated')\n",
        "clear_folder_content(rootdir+'BERT/Subjectivity-Embeddings-Truncated')"
      ],
      "metadata": {
        "id": "xNY4e6GxTfAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_folder_content(rootdir+'BERT/Subjectivity-Embeddings-Truncated-Raw')\n",
        "clear_folder_content(rootdir+'BERT/Subjectivity-Embeddings-Truncated-Raw')"
      ],
      "metadata": {
        "id": "qKVv1s6lButM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MUH7IrD3kNff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embs(embs: List[torch.Tensor], dest_folder: str, kind: str) -> None:\n",
        "    for i in tqdm(range(20)):\n",
        "        to_save = []\n",
        "        data = embs[i*100:(i+1)*100]\n",
        "        for b in data: # getting one tensor at a time -> embs[i]\n",
        "            #for s in b: # to have (512x768)\n",
        "                #print(f\"Print s {s} \\n\")\n",
        "            to_save.append(b[0].cpu().numpy())\n",
        "        np.save(rootdir+'BERT/{}/bert_emb_{}_{}.npy'.format(dest_folder, kind, (i+1)*100), to_save)"
      ],
      "metadata": {
        "id": "tPpBF7fcNhY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_embs = embeddings(dataset=movie_rews_clean, tokenizer=tokenizer, model=model, maxlen=512)"
      ],
      "metadata": {
        "id": "aAjWkH_8XCKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_embs_raw = embeddings(dataset=movie_rews_raw, tokenizer=tokenizer, model=model, maxlen=512)"
      ],
      "metadata": {
        "id": "WyyJFetqI2nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "g-Xzdt8N1yfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90184d7-ab20-41d5-ce78-a4f0501a1eef"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24804"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subj_embs = embeddings(dataset=subj_obj_dataset, tokenizer=tokenizer, model=model, maxlen=45)"
      ],
      "metadata": {
        "id": "JXE6ua4FfjSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mr_embs))"
      ],
      "metadata": {
        "id": "mQpaW6KyaP5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_embs[0].size()"
      ],
      "metadata": {
        "id": "3PGEAkAPaYSD",
        "outputId": "dcb7fa31-2ab6-4713-c094-01365a8e8d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 45, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mr_embs[0][0].size()"
      ],
      "metadata": {
        "id": "Lqm6vFQJacmC",
        "outputId": "781012c2-2864-4e35-ad4f-165ec5f0896a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([45, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mr_embs[0][0] == mr_embs[0]"
      ],
      "metadata": {
        "id": "cTXcddwnaiKw",
        "outputId": "3c79d22f-9eb2-4681-d0bd-52f66ab7476c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[True, True, True,  ..., True, True, True],\n",
              "         [True, True, True,  ..., True, True, True],\n",
              "         [True, True, True,  ..., True, True, True],\n",
              "         ...,\n",
              "         [True, True, True,  ..., True, True, True],\n",
              "         [True, True, True,  ..., True, True, True],\n",
              "         [True, True, True,  ..., True, True, True]]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_embs(embs=mr_embs, dest_folder='Polarity-Embeddings-Truncated', kind='pol')"
      ],
      "metadata": {
        "id": "heSvGXmY3sF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0719c900-2d25-4531-ba68-3f2ba3059e12"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:01<00:00, 13.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_embs(embs=mr_embs_raw, dest_folder='Polarity-Embeddings-Truncated-Raw', kind='pol')"
      ],
      "metadata": {
        "id": "4oLVXR7OJML2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270e4053-11ec-4615-c25b-6d3616e328f7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:01<00:00, 10.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_embs(embs=subj_embs, dest_folder='Subjectivity-Embeddings-Truncated', kind='subj')"
      ],
      "metadata": {
        "id": "mXbO_f8S33NA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d294399-4b93-4ed8-a217-e126b6f07dc2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  8.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_emb_100 = np.load(rootdir+'BERT/Polarity-Embeddings-Truncated/bert_emb_pol_100.npy')\n",
        "bert_emb_200 = np.load(rootdir+'BERT/Polarity-Embeddings-Truncated/bert_emb_pol_200.npy')"
      ],
      "metadata": {
        "id": "1bRWz5nupX8o"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_emb_100.shape"
      ],
      "metadata": {
        "id": "5Kkqfe00Z-1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb714e7-c124-47af-b469-43ade112abba"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 45, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_emb_200.shape"
      ],
      "metadata": {
        "id": "yUFdrYMDaLco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101dc0dd-c7d5-4a10-acf3-816bcf121418"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 45, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_emb_100[:5]==bert_emb_200[:5]"
      ],
      "metadata": {
        "id": "jZ9XoG6vg0Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "iQWu_AXR756n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f94dd67-3a17-4587-c71b-23500fa64fc7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unused"
      ],
      "metadata": {
        "id": "edofTPWuUwUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ids_dataloader, _ = batching_data(dataset=movie_reviews, tokenizer=tokenizer)\n",
        "#for idx, ids in enumerate(ids_dataloader):\n",
        "#    print(idx, ids, len(ids))\n",
        "#    if idx==2:\n",
        "#        break"
      ],
      "metadata": {
        "id": "W1PnRA0lp9aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding taken from last layer of BERT\n",
        "# avoid touching and computing gradients -> torch.no_grad()\n",
        "# https://towardsdatascience.com/what-is-npy-files-and-why-you-should-use-them-603373c78883\n",
        "\n",
        "def fine_tune_BERT( model: BertModel, dataset: pd.DataFrame, tokenizer: BertTokenizer,\n",
        "                   sentence_column: str='text', batch_size: int=BATCH_SIZE, \n",
        "                   device: str=device, rootdir: str=rootdir, filename: str='pol') -> List[torch.Tensor]:\n",
        "    \"\"\"Return Embeddings of dataset\n",
        "\n",
        "        Params:\n",
        "        ------\n",
        "            model: transformers.models.bert.modeling_bert.BertModel\n",
        "                BertModel to get embeddings from\n",
        "            dataset : pd.Dataframe\n",
        "                dataframe containing sentences to embed\n",
        "            tokenizer : transformers.models.bert.tokenization_bert.BertTokenizer\n",
        "                tokenizer model to use\n",
        "            sentence_column : str\n",
        "                column of 'dataset' to get\n",
        "            batch_size : int\n",
        "                batch size\n",
        "            device : str\n",
        "                device to load data on\n",
        "            rootdir : str\n",
        "                root directory where to store embeddings\n",
        "            filename : str\n",
        "                name on which saving embeddings\n",
        "        \n",
        "        Return:\n",
        "        ------\n",
        "            List of embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    embs = []\n",
        "    # getting dataloaders\n",
        "    ids_dataloader, msk_dataloader = batching_data(dataset=dataset, tokenizer=tokenizer, batch_size=batch_size)\n",
        "    # disabling gradients computation --> I'm using a pre-trained net. Don't want to rewrite weights\n",
        "    with torch.no_grad():\n",
        "        # iterating through batches\n",
        "        for idx, (ids,msk) in enumerate(zip(ids_dataloader,msk_dataloader)):\n",
        "\n",
        "            # move data to device \n",
        "            ids = ids.to(device)\n",
        "            msk = msk.to(device)\n",
        "\n",
        "            # 'forward pass'\n",
        "            outputs = model(ids,msk)\n",
        "\n",
        "            # extracting tensor at last layer : https://github.com/esrel/NLU.Lab.2022.Public/blob/master/notebooks/10_sequence_nn.ipynb\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "            print(f\"Done batch #{idx}\")\n",
        "            print(last_hidden_state.cpu().numpy().shape)\n",
        "            embs.append(last_hidden_state)\n",
        "        if len(embs)>0:\n",
        "            print('Embs list has been successfully built')\n",
        "        else:\n",
        "            raise ValueError('List has not been filled')\n",
        "\n",
        "    return embs"
      ],
      "metadata": {
        "id": "2qYdI6k4Qir3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subj_obj_embs = np.load(rootdir+'subj_obj_10000.npy')\n",
        "#for i in range(1000, 11000, 1000):\n",
        "#    subj_obj_embs = np.concatenate((subj_obj_embs, np.load(rootdir+f'subj_obj_{i}.npy')), axis=0)\n",
        "\n",
        "#save_embs(dataset=subj_obj_embs, filename='subj_obj_embs.npy')"
      ],
      "metadata": {
        "id": "wdoXzX1pxvqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save_embs(embs=embs_per_batch_subj_obj, filename='subj_obj.npy')"
      ],
      "metadata": {
        "id": "XTgTB31ljeLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subj_obj = {}\n",
        "\n",
        "# 64*15+40 = 1000\n",
        "#for i in range(1000,11000,1000):\n",
        "    #subj_obj[str(i)] = subj_obj_dataset[i-1000:i]\n",
        "    #embs_per_batch_subj_obj = fine_tune_BERT(model=model, dataset=subj_obj_dataset[i-1000:i], tokenizer=tokenizer)\n",
        "    #save_embs(embs_per_batch_subj_obj, f'subj_obj_{i}.npy')"
      ],
      "metadata": {
        "id": "ozLXZGFdubXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save_embs(embs=mr_embs, filename='pol_embs.npy')"
      ],
      "metadata": {
        "id": "SK-f7LrcFVQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save_embs(embs=subj_embs, filename='subj_obj_embs.npy')"
      ],
      "metadata": {
        "id": "LlHUeyek7S5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}