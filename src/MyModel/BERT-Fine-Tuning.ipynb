{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6xA7BrhPLU3lZr27VxQW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/src/MyModel/BERT-Fine-Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Embedding Fine Tuning"
      ],
      "metadata": {
        "id": "gxa6J-lH4jrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Pu3CpB90ga",
        "outputId": "c89cc908-15d0-4f63-89c9-737f9da29864"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFjx3Xf38J7l",
        "outputId": "0cfcc79e-f01d-47ea-9044-125fc77ae7ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rootdir = '/content/gdrive/MyDrive/Colab Notebooks/Polarity-Subjectivity-Detection/'"
      ],
      "metadata": {
        "id": "suw-3tTbAfrA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Get the GPU device name\n",
        "device = tf.test.gpu_device_name()\n",
        "\n",
        "if 'GPU' in device:\n",
        "  print(f'GPU available : {device}')\n",
        "  device = torch.device('cuda'+device[-2:])\n",
        "else :\n",
        "  device = torch.device(\"cpu\")\n",
        "  raise SystemError(\"GPU not found, use CPU instead\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl20NCMY6_xi",
        "outputId": "e42f9d32-5e0e-4e8c-aad5-80225974f686"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available : /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "p6buUIlod0K5",
        "outputId": "359855a6-b415-430a-e765-02369ee82cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# loading dataset\n",
        "movie_reviews = pd.read_csv(rootdir+'movie_rews.csv')\n",
        "subj_obj_dataset = pd.read_csv(rootdir+'subj_obj_dataset.csv')"
      ],
      "metadata": {
        "id": "TZCUmC4h7WEh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mDrScMj7Asm3",
        "outputId": "7dde0281-adda-4e34-8e98-71e8bb1a8cec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                               text  pos  neg\n",
              "0              0  films adapted comic books plenty success wheth...    1    0\n",
              "1              1  every movie comes along suspect studio every i...    1    0\n",
              "2              2  got mail works alot better deserves order make...    1    0\n",
              "3              3  jaws rare film grabs attention shows single im...    1    0\n",
              "4              4  moviemaking lot like general manager nfl team ...    1    0\n",
              "...          ...                                                ...  ...  ...\n",
              "1995        1995  anything stigmata taken warning releasing simi...    0    1\n",
              "1996        1996  john boorman zardoz goofy cinematic debacle fu...    0    1\n",
              "1997        1997  kids hall acquired taste took least season wat...    0    1\n",
              "1998        1998  time john carpenter great horror director cour...    0    1\n",
              "1999        1999  two party guys bob heads haddaway dance hit lo...    0    1\n",
              "\n",
              "[2000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79473c16-a1a7-4a11-b29b-25f88df65b3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>films adapted comic books plenty success wheth...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>every movie comes along suspect studio every i...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>got mail works alot better deserves order make...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>jaws rare film grabs attention shows single im...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>moviemaking lot like general manager nfl team ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>anything stigmata taken warning releasing simi...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>john boorman zardoz goofy cinematic debacle fu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>kids hall acquired taste took least season wat...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>time john carpenter great horror director cour...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>two party guys bob heads haddaway dance hit lo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79473c16-a1a7-4a11-b29b-25f88df65b3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79473c16-a1a7-4a11-b29b-25f88df65b3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79473c16-a1a7-4a11-b29b-25f88df65b3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subj_obj_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DujGO_7-AwXH",
        "outputId": "6d5581ba-1bab-4593-b622-42c32e4829ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                               text   tag\n",
              "0              0  smart and alert , thirteen conversations about...  subj\n",
              "1              1  color , musical bounce and warm seas lapping o...  subj\n",
              "2              2  it is not a mass-market entertainment but an u...  subj\n",
              "3              3  a light-hearted french film about the spiritua...  subj\n",
              "4              4  my wife is an actress has its moments in looki...  subj\n",
              "...          ...                                                ...   ...\n",
              "9995        9995  in the end , they discover that balance in lif...   obj\n",
              "9996        9996  a counterfeit 1000 tomin bank note is passed i...   obj\n",
              "9997        9997  enter the beautiful and mysterious secret agen...   obj\n",
              "9998        9998  after listening to a missionary from china spe...   obj\n",
              "9999        9999  looking for a short cut to fame , glass concoc...   obj\n",
              "\n",
              "[10000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbb02797-4a7d-4e0e-83e3-ea891ffde3ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>smart and alert , thirteen conversations about...</td>\n",
              "      <td>subj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>color , musical bounce and warm seas lapping o...</td>\n",
              "      <td>subj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>it is not a mass-market entertainment but an u...</td>\n",
              "      <td>subj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>a light-hearted french film about the spiritua...</td>\n",
              "      <td>subj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>my wife is an actress has its moments in looki...</td>\n",
              "      <td>subj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>in the end , they discover that balance in lif...</td>\n",
              "      <td>obj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
              "      <td>obj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>enter the beautiful and mysterious secret agen...</td>\n",
              "      <td>obj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>after listening to a missionary from china spe...</td>\n",
              "      <td>obj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>looking for a short cut to fame , glass concoc...</td>\n",
              "      <td>obj</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbb02797-4a7d-4e0e-83e3-ea891ffde3ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bbb02797-4a7d-4e0e-83e3-ea891ffde3ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bbb02797-4a7d-4e0e-83e3-ea891ffde3ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Major commands :\n",
        "- .tokenize(sent)\n",
        "- .convert_tokens_to_ids(tokenized_sent)\n",
        "- .encode.plus() [source](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus)"
      ],
      "metadata": {
        "id": "RnYxmD2FEySp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT model script from: huggingface.co\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import Tuple, List, Dict\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import logging\n",
        "\n",
        "# to not see warning everytime\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "KuyfsCFkKaSy"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "id": "e3FmCxeBsaD0",
        "outputId": "8d1e092c-cba3-4e38-a882-1d91429cab1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.bert.modeling_bert.BertModel"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(dataset : pd.DataFrame, sentence_column : str) -> Tuple[List]:#Tuple[tf.Tensor]:\n",
        "    embeddings = {\n",
        "        'embedding' : [],\n",
        "        'attention_mask' : []\n",
        "        }\n",
        "        \n",
        "    for sent in dataset[sentence_column]:\n",
        "        dic_sent_encoding = tokenizer.encode_plus(sent, # untokenized sentence\n",
        "                                                add_special_tokens = True,  # add '[CLS]' and '[SEP]'\n",
        "                                                truncation = True,  # truncate to maximum length\n",
        "                                                padding = \"max_length\",  # pad to maximum admissible sentence\n",
        "                                                return_attention_mask = True,  # return attention mask\n",
        "                                                return_tensors = \"pt\") # returns tensorflow constant obj\n",
        "    \n",
        "        # extracting embeddings and attention masks in list form\n",
        "        embeddings['embedding'].append(dic_sent_encoding['input_ids'])\n",
        "        embeddings['attention_mask'].append(dic_sent_encoding['attention_mask'])\n",
        "\n",
        "    # convert lists of tensors into tensors\n",
        "    input_ids = torch.cat(embeddings['embedding'], axis=0)\n",
        "    attention_masks = torch.cat(embeddings['attention_mask'], axis=0)\n",
        "    \n",
        "    return input_ids, attention_masks\n",
        "    #return embeddings['embedding'], embeddings['attention_mask']"
      ],
      "metadata": {
        "id": "I1Ov7l-CAyG6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_mr, attention_masks_mr = embedding(dataset=movie_reviews, sentence_column='text')"
      ],
      "metadata": {
        "id": "bslSQQ1VagQf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_mr[0]"
      ],
      "metadata": {
        "id": "98aw4RsqMvf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks_mr[0]"
      ],
      "metadata": {
        "id": "0Wa8BAy3SaQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_subj_obj , attention_masks_subj_obj = embedding(dataset=subj_obj_dataset, sentence_column='text')"
      ],
      "metadata": {
        "id": "A_RI20fmM0JW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_subj_obj[0]"
      ],
      "metadata": {
        "id": "QmB8kLzASrTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks_subj_obj[0]"
      ],
      "metadata": {
        "id": "pPay8ed6Stin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ids_mr))\n",
        "print(type(ids_mr[0]))"
      ],
      "metadata": {
        "id": "08HxkZp3kkI_",
        "outputId": "2e33d861-f95c-4f17-e699-87d09565a796",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "uNy-pqdEnS-h"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batching_data(dataset: pd.DataFrame, column_name: str ='text', batch_size: int=BATCH_SIZE) -> Tuple[torch.utils.data.DataLoader]:\n",
        "    '''Function to batch sentences to fit BERT model and get embeddings\n",
        "\n",
        "        Params:\n",
        "        ------\n",
        "            dataset : pd.DataFrame\n",
        "                dataset to be batched\n",
        "            column_name : str\n",
        "                column of 'dataset' DataFrame to catch \n",
        "            batch_size : int\n",
        "                batch size\n",
        "        \n",
        "        Return:\n",
        "        ------\n",
        "            tuple of dataloader to iterate over\n",
        "    '''\n",
        "\n",
        "    # getting encodings and attention masks for whole dataset\n",
        "    ids, msk = embedding(dataset=dataset, sentence_column=column_name)\n",
        "    # defining DataLoader that batches ids randomly\n",
        "    ids_dataloader = DataLoader(dataset=ids, sampler=RandomSampler(ids), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # defining DataLoader that batches masks\n",
        "    msk_dataloader = DataLoader(dataset=msk, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return ids_dataloader, msk_dataloader"
      ],
      "metadata": {
        "id": "16S6YJ7wnBZ5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataloader, _ = batching_data(dataset=movie_reviews)\n",
        "for idx, ids in enumerate(ids_dataloader):\n",
        "    print(idx, ids, len(ids))"
      ],
      "metadata": {
        "id": "W1PnRA0lp9aV",
        "outputId": "5e1246c9-3108-4fd9-d61e-9c3b8a574425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([[  101,  3152,  5967,  ...,  3533, 14913,   102],\n",
            "        [  101,  2296,  3185,  ...,     0,     0,     0],\n",
            "        [  101,  2288,  5653,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 20705,  2419,  ...,     0,     0,     0],\n",
            "        [  101,  2360,  5199,  ...,  2290,  3185,   102],\n",
            "        [  101,  3923,  5722,  ...,  4066,  3291,   102]]) 128\n",
            "1 tensor([[  101, 18269,  2310,  ...,     0,     0,     0],\n",
            "        [  101,  2293,  2155,  ...,     0,     0,     0],\n",
            "        [  101,  3666, 14008,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2204,  5933,  ...,     0,     0,     0],\n",
            "        [  101, 12731,  4103,  ..., 12731,  4103,   102],\n",
            "        [  101,  2411,  2056,  ...,     0,     0,     0]]) 128\n",
            "2 tensor([[  101,  2111,  4906,  ...,  2744,  7616,   102],\n",
            "        [  101,  3160,  2356,  ...,     0,     0,     0],\n",
            "        [  101, 21072, 10338,  ...,  7601,  4183,   102],\n",
            "        ...,\n",
            "        [  101,  6812, 20349,  ..., 23176,  7828,   102],\n",
            "        [  101, 16016,  4349,  ..., 13156,  2100,   102],\n",
            "        [  101,  2228,  2034,  ...,  2290,  2256,   102]]) 128\n",
            "3 tensor([[  101,  2113,  2525,  ...,     0,     0,     0],\n",
            "        [  101, 18269,  2143,  ...,     0,     0,     0],\n",
            "        [  101, 11721,  5946,  ...,  5399, 10021,   102],\n",
            "        ...,\n",
            "        [  101,  4339,  3319,  ...,  6919,  7812,   102],\n",
            "        [  101,  5694,  6580,  ...,     0,     0,     0],\n",
            "        [  101, 18568,  5470,  ...,  2290,  9415,   102]]) 128\n",
            "4 tensor([[  101,  2089,  4025,  ...,  5254,  2638,   102],\n",
            "        [  101,  2034,  2657,  ...,     0,     0,     0],\n",
            "        [  101,  1047, 29443,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2472, 18641,  ...,     0,     0,     0],\n",
            "        [  101,  3019,  2141,  ...,  5190,  2086,   102],\n",
            "        [  101,  5416,  2186,  ...,  2131,  4206,   102]]) 128\n",
            "5 tensor([[  101,  2823,  6945,  ...,     0,     0,     0],\n",
            "        [  101,  2198,  2360,  ...,  2290, 21863,   102],\n",
            "        [  101,  5436,  2184,  ...,  3170,  3672,   102],\n",
            "        ...,\n",
            "        [  101,  3342,  3773,  ...,  3238,  6424,   102],\n",
            "        [  101, 19461,  2265,  ...,     0,     0,     0],\n",
            "        [  101,  5261,  5954,  ...,     0,     0,     0]]) 128\n",
            "6 tensor([[  101,  2296,  2156,  ...,     0,     0,     0],\n",
            "        [  101,  2273,  2304,  ...,  2972,  2143,   102],\n",
            "        [  101,  6798, 10882,  ...,  2613,  3894,   102],\n",
            "        ...,\n",
            "        [  101,  2292,  2330,  ...,  2290, 11320,   102],\n",
            "        [  101, 17081, 19739,  ...,     0,     0,     0],\n",
            "        [  101,  3548,  2093,  ...,  2290,  6625,   102]]) 128\n",
            "7 tensor([[  101,  3021, 25805,  ...,  2290,  2059,   102],\n",
            "        [  101,  2442,  6449,  ...,  2638,  2290,   102],\n",
            "        [  101, 21014,  2544,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  5544,  2453,  ...,  2103,  3853,   102],\n",
            "        [  101,  7324,  3599,  ...,  3496,  3158,   102],\n",
            "        [  101,  7488,  2159,  ...,  2228,  2052,   102]]) 128\n",
            "8 tensor([[  101,  9641,  9016,  ...,  4371, 23067,   102],\n",
            "        [  101,  2028,  2453,  ...,     0,     0,     0],\n",
            "        [  101,  2637,  7459,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2761,  2387,  ...,     0,     0,     0],\n",
            "        [  101,  3744,  8149,  ...,     0,     0,     0],\n",
            "        [  101, 13132,  4654,  ...,     0,     0,     0]]) 128\n",
            "9 tensor([[  101,  2763,  2204,  ...,     0,     0,     0],\n",
            "        [  101,  3419,  4487,  ...,     0,     0,     0],\n",
            "        [  101, 14997,  8942,  ...,  3305,  2524,   102],\n",
            "        ...,\n",
            "        [  101,  2919,  3152,  ...,     0,     0,     0],\n",
            "        [  101,  2052,  2524,  ...,  4515,  2638,   102],\n",
            "        [  101,  2364,  3291,  ...,     0,     0,     0]]) 128\n",
            "10 tensor([[  101, 18269, 11690,  ...,     0,     0,     0],\n",
            "        [  101,  2307,  3364,  ...,  2290, 13638,   102],\n",
            "        [  101, 18458, 29083,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  7779, 29058,  ...,     0,     0,     0],\n",
            "        [  101,  2145,  3275,  ...,     0,     0,     0],\n",
            "        [  101,  2175, 17080,  ...,  2638,  2290,   102]]) 128\n",
            "11 tensor([[  101,  3369,  3000,  ...,     0,     0,     0],\n",
            "        [  101, 18269,  3565,  ...,     0,     0,     0],\n",
            "        [  101,  3342,  3419,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2562,  4658,  ...,  2553, 13534,   102],\n",
            "        [  101,  2348,  2042,  ...,     0,     0,     0],\n",
            "        [  101, 20228, 16814,  ...,  2290,  2359,   102]]) 128\n",
            "12 tensor([[  101,  3319,  3397,  ..., 13124,  2444,   102],\n",
            "        [  101, 10392,  3973,  ...,     0,     0,     0],\n",
            "        [  101,  3185,  8474,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 12633,  2130,  ...,     0,     0,     0],\n",
            "        [  101, 18269,  2274,  ...,     0,     0,     0],\n",
            "        [  101,  6045,  2232,  ...,  2290,  2039,   102]]) 128\n",
            "13 tensor([[  101,  6654,  3428,  ...,     0,     0,     0],\n",
            "        [  101, 19962, 22599,  ...,     0,     0,     0],\n",
            "        [  101,  3940,  3152,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  6530,  6198,  ..., 27427,  5313,   102],\n",
            "        [  101,  2637,  5440,  ...,     0,     0,     0],\n",
            "        [  101,  2034,  2143,  ...,     0,     0,     0]]) 128\n",
            "14 tensor([[  101,  2204, 23259,  ...,     0,     0,     0],\n",
            "        [  101,  5409,  3185,  ...,     0,     0,     0],\n",
            "        [  101, 19962, 22599,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2745, 18091,  ...,  2158,  9744,   102],\n",
            "        [  101,  2113,  2242,  ...,  5861,  2818,   102],\n",
            "        [  101,  3054,  3865,  ...,     0,     0,     0]]) 128\n",
            "15 tensor([[  101,  6294, 18203,  ...,     0,     0,     0],\n",
            "        [  101,  3602,  2089,  ...,  2090,  2638,   102],\n",
            "        [  101, 13498,  5643,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  4268,  2534,  ...,     0,     0,     0],\n",
            "        [  101,  2051,  2198,  ...,     0,     0,     0],\n",
            "        [  101,  2048,  2283,  ...,     0,     0,     0]]) 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding taken from last layer of BERT\n",
        "# avoid touching and computing gradients\n",
        "# https://towardsdatascience.com/what-is-npy-files-and-why-you-should-use-them-603373c78883\n",
        "\n",
        "def fine_tune_BERT(model, dataset: pd.DataFrame, sentence_column: str, batch_size: int=BATCH_SIZE, device: str=device) -> None:\n",
        "    \"\"\"Return Embeddings of dataset\n",
        "\n",
        "        Params:\n",
        "        ------\n",
        "            model: transformers.models.bert.modeling_bert.BertModel\n",
        "                BertModel to get embeddings from\n",
        "            dataset : pd.Dataframe\n",
        "                dataframe containing sentences to embed\n",
        "            sentence_column : str\n",
        "                column of 'dataset' to get\n",
        "            batch_size : int\n",
        "                batch size\n",
        "        \n",
        "        Return:\n",
        "        ------\n",
        "            Nothing\n",
        "    \"\"\"\n",
        "\n",
        "    embs = []\n",
        "    # getting dataloaders\n",
        "    ids_dataloader, msk_dataloader = batching_data(dataset=dataset)\n",
        "    # disabling gradients computation --> I'm using a pre-trained net. Don't want to rewrite weights\n",
        "    with torch.no_grad():\n",
        "        # iterating through batches\n",
        "        for idx, (ids,msk) in enumerate(zip(ids_dataloader,msk_dataloader)):\n",
        "\n",
        "            # move data to device \n",
        "            ids = ids.to(device)\n",
        "            msk = msk.to(device)\n",
        "\n",
        "            # 'forward pass'\n",
        "            outputs = model(ids,msk)\n",
        "\n",
        "            # extracting tensor at last layer : https://github.com/esrel/NLU.Lab.2022.Public/blob/master/notebooks/10_sequence_nn.ipynb\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "2qYdI6k4Qir3",
        "outputId": "c50adb1d-9afa-47a8-aa6c-bdfad7c0242f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e3b8d57ee44b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# avoid touching and computing gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks_mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.93 GiB (GPU 0; 14.76 GiB total capacity; 12.15 GiB already allocated; 1.59 GiB free; 12.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9a_OQRmRQR1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}