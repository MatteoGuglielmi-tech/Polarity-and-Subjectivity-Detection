{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM63yf8GKRweuD+geGCw/5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoGuglielmi-tech/Polarity-and-Subjectivity-Detection/blob/main/NTN_no_print.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WlxO-pC-Pmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralTensorNetwork(nn.Module):\n",
        "    def __init__(self, output_dim: int, input_dim: int, activation: str=\"tanh\", mean: float=0.0, std: float=1.0):\n",
        "        \n",
        "        super(NeuralTensorNetwork, self).__init__()\n",
        "\n",
        "        # setting input and output dimensions\n",
        "        self.k = output_dim\n",
        "        self.d = input_dim # e1,e2\n",
        "\n",
        "        # setting mean and std for random initialization\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        # parameters has been used in order to consider W, V, b as model parameters\n",
        "        # inference -> they'll be optimized\n",
        "\n",
        "        # normal sampling -> https://pytorch.org/docs/stable/generated/torch.normal.html\n",
        "        # parameter -> https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
        "        self.W = nn.Parameter(torch.normal(self.mean, self.std, size=(self.k, self.d, self.d)))\n",
        "        self.V = nn.Parameter(torch.normal(self.mean, self.std, size=(2*self.d, self.k)))\n",
        "        self.b = nn.Parameter(torch.zeros(size=(self.d,)))\n",
        "        \n",
        "        #print(f\"self.W : {torch.Tensor.size(self.W)}\")\n",
        "        #print(f\"self.V : {torch.Tensor.size(self.V)}\")\n",
        "        #print(f\"self.b : {torch.Tensor.size(self.b)}\")\n",
        "        \n",
        "        if activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif self.activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        # checking for a good activation function\n",
        "        else:\n",
        "            raise ValueError('Possible activation choices are tanh, sigmoid or ReLU')\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n",
        "\n",
        "        # getting the entities\n",
        "        e1 = inputs[0]\n",
        "        e2 = inputs[1]\n",
        "        #print(f\"e1.shape : {torch.Tensor.size(e1)}\")\n",
        "        #print(f\"e2.shape : {torch.Tensor.size(e2)}\")\n",
        "        #print(f\"self.W[0] : {torch.Tensor.size(self.W[0])}\")\n",
        "        #print(f\"self.V : {torch.Tensor.size(self.V)}\")\n",
        "        #print(f\"self.bias : {torch.Tensor.size(self.b)}\")\n",
        "        #print(f\"batch_size : {torch.clone(e1).cpu().numpy().shape[0]}\")\n",
        "        #print(f\"k : {self.k}\")\n",
        "        #print(f\"torch.cat([e1,e2], dim=1) : {torch.Tensor.size(torch.cat([e1,e2], dim=1))}\")\n",
        "        \n",
        "\n",
        "        # input tensor should be of shape (batch_size, padded_length, 768)\n",
        "        batch_size = torch.clone(e1).cpu().numpy().shape[0]\n",
        "        k = self.k\n",
        "        d = self.d\n",
        "\n",
        "        # print(f\"dot prod : {torch.matmul(e1, self.W[0])}\")\n",
        "        #print(f\"dot prod size: {torch.Tensor.size(torch.matmul(e1, self.W[0]))}\")\n",
        "\n",
        "        # bilinear tensor + bias\n",
        "        bil_bias = [torch.sum((e2 * torch.matmul(e1, self.W[0])) + self.b, axis=1)]\n",
        "        \n",
        "        for i in range(1,k):\n",
        "            bil_bias.append(torch.sum((e2*torch.matmul(e1, self.W[i])) + self.b, axis=1))\n",
        "        \n",
        "        bil_bias = torch.cat(bil_bias, axis=0)\n",
        "        bil_bias = torch.reshape(bil_bias, (batch_size, k))\n",
        "\n",
        "        # Vr * [e1, e2]\n",
        "        rest = torch.matmul(torch.cat([e1,e2], dim=1), self.V)\n",
        "\n",
        "        e1_R_e2 = bil_bias + rest\n",
        "\n",
        "        # applying activation\n",
        "        f = self.activation(e1_R_e2)\n",
        "        return f"
      ],
      "metadata": {
        "id": "-anTmLwuTLvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}